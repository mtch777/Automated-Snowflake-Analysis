{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cb74f2-a17f-4034-859f-bd39299c8383",
   "metadata": {},
   "source": [
    "## Init Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09ba44-795e-4a7e-9d5d-dc4b7da81af9",
   "metadata": {},
   "source": [
    "#### - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b707cff-acef-4e81-8414-b4e4d0533cbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------__Init__Import Packages-------------------------------------\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, count, lit, avg, mean, length, min, median, max, stddev, when, mode, approx_percentile, datediff, dateadd, to_date, date_from_parts, unix_timestamp, from_unixtime, collect_set\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from snowflake.snowpark.types import StringType, FloatType, IntegerType, DateType, BooleanType, DecimalType, VariantType, StructType, StructField, LongType\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "from time import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import ast\n",
    "from pprint import pprint \n",
    "import matplotlib.pyplot as plt\n",
    "from snowflake.snowpark import DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7961f47-2d96-4449-a628-7f0fac290313",
   "metadata": {},
   "source": [
    "#### - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4553d6b7-c99e-4afc-bbfb-9c8bc4a3a019",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------__Init__Initialize Parameters-------------------------------------\n",
    "\n",
    "#__Needed from previous functions__\n",
    "\n",
    "#__Set for the first time in this function__\n",
    "\n",
    "#--get_user_inputs--\n",
    "connection_parameters = None\n",
    "restart_session_boolean = None\n",
    "reload_data_boolean = None\n",
    "enroll_data_table_name = None #change to raw_source_name\n",
    "col_analysis_table_name = None\n",
    "raw_enroll_data_table_name = None\n",
    "'''add col_analysis_table_name'''\n",
    "'''add database & schema'''\n",
    "\n",
    "\n",
    "#--get_or_create_session--\n",
    "session_exists = False  # \n",
    "session = None\n",
    "\n",
    "#--load_data--\n",
    "enroll_stable_copy = None #change to raw_source_copy\n",
    "enroll_stable_copy_schema = None #change to raw_source_schema\n",
    "dataframe_initialized = False\n",
    "dtype_to_class_name = None\n",
    "\n",
    "#--map_data_types_to_categories--\n",
    "schema_mapped_df = None\n",
    "\n",
    "#--run_column_tests--\n",
    "test_results_df = None\n",
    "\n",
    "#--upload_column_tests_to_snowflake--\n",
    "column_analysis = None\n",
    "snowflake_updated_column_analysis = None\n",
    "\n",
    "#--classify_columns_for_corr_and_anova--\n",
    "categorical_corr_anova_threshold = None\n",
    "main_target_column = None\n",
    "NUMERIC_COLUMN_NAMES = None\n",
    "CATEGORICAL_COLUMN_NAMES = None\n",
    "CATEGORICAL_COLUMN_NAMES_plus_goal = None\n",
    "IGNORE_COLUMN_NAMES = None\n",
    "\n",
    "#--convert_dates_to_unix_encoding--\n",
    "raw_data_snowpark_copy = None\n",
    "DATES_TO_CONVERT_TO_NUMERIC = None\n",
    "\n",
    "#--sort_category_values_by_goal_metric--\n",
    "sorted_category_averages = None\n",
    "\n",
    "#--convert_category_values_to_encoding--\n",
    "encoded_df = None\n",
    "binary_encoded_df = None\n",
    "\n",
    "#--run_standard_encoded_corr_analysis--\n",
    "standard_pearson_corr_results = None\n",
    "\n",
    "#--run_binary_encoded_corr_analysis--\n",
    "binary_pearson_corr_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f99737-2898-4496-a541-3e07e94c0cc1",
   "metadata": {},
   "source": [
    "#### - Custom Debug Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8acfa-25e7-4fdb-a078-07739cd6472e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------ \"Peek\" Debug Tool (\"\",\"v\",\"p\"...\"3p\") -------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Adjusted peek function to handle Snowpark DataFrames as well\n",
    "def peek(scope=\"1\"):\n",
    "    # Mapping the scope number to the structure's keys\n",
    "    scope_map = {\"1\": \"direct\", \"2\": \"context\", \"3\": \"general\"}\n",
    "\n",
    "    # Determine whether we are looking for variables or parameters\n",
    "    section_suffix = {\n",
    "        \"v\": \"variables\",\n",
    "        \"p\": \"parameters\"\n",
    "    }\n",
    "    section = section_suffix.get(scope[-1].lower(), None)\n",
    "    \n",
    "    # Check if we're dealing with sub-sections or the entire section\n",
    "    if section:\n",
    "        scope_key = f\"{scope_map.get(scope[0])}_{section}\"\n",
    "    else:\n",
    "        scope_key = f\"{scope_map.get(scope)}_{section_suffix['v']}\"  # Default to variables if not specified\n",
    "\n",
    "    print(scope_key)\n",
    "    \n",
    "    # Fetch the values from the global scope\n",
    "    global_variables = globals()\n",
    "\n",
    "    # Helper function to limit output based on type\n",
    "    def limited_output(value):\n",
    "        # Check for Snowpark DataFrame and convert to Pandas DataFrame\n",
    "        if isinstance(value, DataFrame):\n",
    "            return display(value.to_pandas().iloc[:5, :5])#.to_string(index=False)\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            return display(value.iloc[:5, :5])#.to_string(index=False)\n",
    "        elif isinstance(value, list):\n",
    "            return ', '.join(map(str, value[:30]))\n",
    "        elif isinstance(value, str):\n",
    "            return value[:30]\n",
    "        elif isinstance(value, dict):\n",
    "            return ', '.join([f\"{k}: {str(v)[:30]}\" for k, v in list(value.items())[:3]])\n",
    "        elif isinstance(value, set):\n",
    "            return ', '.join(map(str, list(value)[:30]))\n",
    "        else:\n",
    "            # Convert to string to handle other data types and limit the output\n",
    "            return str(value)[:30]\n",
    "            \n",
    "    # Print the variable values with output limiting\n",
    "    for var_name in var_param_structure[scope_map.get(scope[0], \"\")][scope_key]:\n",
    "        value = global_variables.get(var_name, 'Variable not found')\n",
    "        limited_val = limited_output(value)\n",
    "        print(f\"{var_name}:\\n{limited_val}\\n\")\n",
    "\n",
    "    output = []\n",
    "    for var_name in var_param_structure[scope_map.get(scope[0], \"\")][scope_key]:\n",
    "        value = global_variables.get(var_name, 'Variable not found')\n",
    "        limited_val = limited_output(value)\n",
    "        output.append(f\"{var_name}:\\n{limited_val}\\n\")\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# The 'structure' variable should be defined in the global scope as provided.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "var_param_structure = {\n",
    "    \"direct\": {\n",
    "        \"direct_variables\": [\"enroll_stable_copy\",\n",
    "                             \"enroll_stable_copy_schema\",\n",
    "                             \"NUMERIC_COLUMN_NAMES\",\n",
    "                             \"CATEGORICAL_COLUMN_NAMES\",\n",
    "                             \"IGNORE_COLUMN_NAMES\",\n",
    "                             \"DATES_TO_CONVERT_TO_NUMERIC\",\n",
    "                             \"sorted_category_averages\",\n",
    "                             \"encoded_df\"],\n",
    "        \"direct_parameters\": [\"enroll_data_table_name\",\n",
    "                              \"col_analysis_table_name\", \n",
    "                              \"Category_Mapping_Reference\",\n",
    "                              \"enroll_stable_copy_schema\"]\n",
    "    },\n",
    "    \"context\": {\n",
    "        \"context_variables\": [\"schema_mapped_df\"],\n",
    "        \"context_parameters\": [\"connection_parameters.database\",\n",
    "                               \"connection_parameters.schema\"]\n",
    "    },\n",
    "    \"general\": {\n",
    "        \"general_variables\": [\"experiment_category_columns\",\n",
    "                              \"experiment_category_list\", \n",
    "                              \"cat_test_results_dict\",\n",
    "                              \"categorical_summary_results_df\"],\n",
    "        \"general_parameters\": [\"GlobalStructureMap_file_name\",\n",
    "                               \"CellStructureMap_file_name\", \n",
    "                               \"CellInteractionMap_file_name\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "#peek(\"1v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9831d8d7-355a-4edf-8be4-3844f84c0efe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------Showall Function-------------------------------------\n",
    "def showall (data, max_rows = 900, max_columns = 120):\n",
    "    pd.options.display.max_columns = max_columns\n",
    "    pd.options.display.max_rows = max_rows\n",
    "    display(data)\n",
    "    pd.options.display.max_columns = 60\n",
    "    pd.options.display.max_rows = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236089e-ac93-441b-a138-d3d92e28b729",
   "metadata": {},
   "source": [
    "#### - Get User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "646655b5-068e-47ce-ab29-8b64be0c8459",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------ Get User Inputs-------------------------------------\n",
    "def get_user_inputs ():\n",
    "    #__Set for the first time in this function__\n",
    "    global restart_session_boolean\n",
    "    global reload_data_boolean\n",
    "    global connection_parameters\n",
    "    global enroll_data_table_name\n",
    "    global col_analysis_table_name\n",
    "    global raw_enroll_data_table_name\n",
    "\n",
    "    #__function logic starts here__\n",
    "    \n",
    "    restart_session_boolean = 'No'\n",
    "    reload_data_boolean  = 'No'\n",
    "    \n",
    "    connection_parameters = {\n",
    "        \"account\": \"NEEDS_POPULATED\",\n",
    "        \"user\": \"NEEDS_POPULATED\",\n",
    "        \"authenticator\": \"externalbrowser\", #.\n",
    "        \"database\": \"NEEDS_POPULATED\",  # Specify your database name here\n",
    "        \"schema\": \"NEEDS_POPULATED\"  # Specify your schema name here\n",
    "        #\"warehouse\": \"your_warehouse_name\",  # Specify if necessary\n",
    "        #\"role\": \"your_role_name\",  # Specify if necessary\n",
    "    }  \n",
    "\n",
    "    #Eventually change to [database].[schema].... format\n",
    "    raw_enroll_data_table_name = 'NEEDS_POPULATED'\n",
    "    \n",
    "    enroll_data_table_name = 'NEEDS_POPULATED'\n",
    "\n",
    "    col_analysis_table_name = \"NEEDS_POPULATED\"\n",
    "\n",
    "    categorical_corr_anova_threshold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22399cf8-0416-4800-a70f-3ade8d84b647",
   "metadata": {},
   "source": [
    "#### - Connections/Integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53cd8718-31b0-4ee5-ab0e-1acbf6f4e89f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------Connect to Snowpark API-------------------------------------\n",
    "def get_or_create_session():\n",
    "    #__Needed from previous functions__\n",
    "    #--get_or_create_session--\n",
    "    global connection_parameters\n",
    "    global restart_session_boolean\n",
    "\n",
    "    #__Set for the first time in this function__\n",
    "    global session_exists\n",
    "    global session\n",
    "    \n",
    "    #___function logic starts here___\n",
    "\n",
    "    if not session_exists or restart_session_boolean == 'Yes':\n",
    "        if not session_exists:\n",
    "            print(\"no existing session, creating new...\")\n",
    "            print(\"\")\n",
    "        elif restart_session_boolean == 'Yes':\n",
    "            print(\"closing existing session, reconnecting...\")\n",
    "            \n",
    "        # Create a new session\n",
    "        print(\"snowpark API response...\")\n",
    "        session = Session.builder.configs(connection_parameters).create()\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"session created\")\n",
    "        print(\"\")\n",
    "\n",
    "        session.use_database(connection_parameters['database'])\n",
    "        session.use_schema(connection_parameters['schema'])\n",
    "        print(\"Database:\",connection_parameters['database'])\n",
    "        print(\"Schema:\",connection_parameters['schema'])\n",
    "\n",
    "        session_exists = True  # Update flag to indicate session now exists\n",
    "        return\n",
    "    else:\n",
    "        print(\"continuing existing session\")\n",
    "        # Use the existing session\n",
    "        from snowflake.snowpark.context import get_active_session\n",
    "        return get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb5c29-b124-4dcb-8ff1-ffc028d333a9",
   "metadata": {},
   "source": [
    "## Load & Pre-Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ed582-d8f2-4cc8-963c-6d5085179241",
   "metadata": {},
   "source": [
    "#### - Convert Potential Date Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475299c1-e15e-4cdd-a34e-239b6108c8f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------ Convert Potential Date Fields  -------------------------------------\n",
    "def convert_potential_date_fields():\n",
    "    print(\"converting potential date fields...\")\n",
    "    string_columns_query = f\"\"\"\n",
    "    SELECT COLUMN_NAME\n",
    "    FROM INFORMATION_SCHEMA.COLUMNS\n",
    "    WHERE TABLE_NAME = 'BI_ENROLL_RATE_PROD_DATA'\n",
    "    AND DATA_TYPE = 'TEXT';\n",
    "    \"\"\"\n",
    "    string_columns_df = session.sql(string_columns_query).collect()\n",
    "    string_column_names = [row['COLUMN_NAME'] for row in string_columns_df]\n",
    "    print(\"string columns pulled\")\n",
    "\n",
    "    select_query = f\"\"\"\n",
    "    SELECT {', '.join(string_column_names)}\n",
    "    FROM \"{raw_enroll_data_table_name}\"\n",
    "    \"\"\"    \n",
    "    df = session.sql(select_query).to_pandas()\n",
    "    print(\"string column data queried\")\n",
    "    \n",
    "    successful_conversions = []\n",
    "    \n",
    "    # Attempt to convert each string column to datetime, raising exceptions for unparseable strings\n",
    "    for column in df.select_dtypes(include=['object']).columns:\n",
    "        try:\n",
    "            # Attempt to convert the column to datetime, raising an exception if it fails\n",
    "            pd.to_datetime(df[column], errors='raise')\n",
    "            # If no exception was raised, the conversion was successful\n",
    "            print(f\"Column '{column}' successfully converted to datetime.\")\n",
    "            successful_conversions.append(column)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    snowpark_df = session.create_dataframe(df)\n",
    "    snowpark_df.write.save_as_table(raw_enroll_data_table_name+\"_temporary\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfc6b7-27ab-469b-9058-c65376b8202d",
   "metadata": {},
   "source": [
    "#### - Normalize Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36514b1-0df1-43a2-9cbe-e678f42e2b86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------ Add Normalized Dates -------------------------------------\n",
    "def add_normalized_dates():\n",
    "    # Define your Stored Procedure creation SQL\n",
    "    create_procedure_sql = \"\"\"\n",
    "    CREATE OR REPLACE PROCEDURE dynamic_date_diff_and_union_save_as_table()\n",
    "    RETURNS STRING\n",
    "    LANGUAGE JAVASCRIPT\n",
    "    EXECUTE AS CALLER\n",
    "    AS\n",
    "    $$\n",
    "    var query = `\n",
    "    SELECT COLUMN_NAME, DATA_TYPE\n",
    "    FROM \"EDW_PROD\".\"INFORMATION_SCHEMA\".\"COLUMNS\"\n",
    "    WHERE TABLE_SCHEMA = 'ANALYTICS'\n",
    "    AND TABLE_NAME = 'BI_ENROLL_RATE_PROD_DATA_TEMPORARY';\n",
    "    `;\n",
    "    // Debug: Print the query to check for errors\n",
    "    console.log(query); \n",
    "    \n",
    "    var resultSet = snowflake.execute({sqlText: query});\n",
    "    var columnPairs = []; // For storing original and normalized date columns together\n",
    "    var numberColumns = [];\n",
    "    var binaryColumns = [];\n",
    "    var booleanColumns = [];\n",
    "    var stringColumns = [];\n",
    "    var arrayColumns = [];\n",
    "    var variantObjectColumns = [];\n",
    "    var geographyColumns = [];\n",
    "    var timeColumns = [];\n",
    "    var otherColumns = [];\n",
    "    \n",
    "    while (resultSet.next()) {\n",
    "        var colName = resultSet.getColumnValue(1);\n",
    "        var dataType = resultSet.getColumnValue(2);\n",
    "    \n",
    "        if (['DATE', 'TIMESTAMP', 'TIMESTAMP_LTZ', 'TIMESTAMP_NTZ', 'TIMESTAMP_TZ'].includes(dataType)) {\n",
    "            // Include the original date column\n",
    "            columnPairs.push(`\"${colName}\"`);\n",
    "            // Include the normalized date column if it's not the reference date column\n",
    "            if (colName !== 'IDENTIFIED_LIST_LOAD_DATE') {\n",
    "                //columnPairs.push(`DATEDIFF(day, IDENTIFIED_LIST_LOAD_DATE, \"${colName}\") AS ${colName.toUpperCase()}_NORMALIZED`);\n",
    "                columnPairs.push(`DATEDIFF(day, IDENTIFIED_LIST_LOAD_DATE, ${colName}) AS \"${colName.toUpperCase()}_NORMALIZED\"`);\n",
    "            }\n",
    "        } else if (['NUMBER', 'FLOAT', 'DECIMAL', 'INTEGER', 'BIGINT', 'SMALLINT', 'NUMERIC'].includes(dataType)) {\n",
    "            numberColumns.push(`\"${colName}\"`);\n",
    "        } else if (dataType === 'BINARY') {\n",
    "            binaryColumns.push(`\"${colName}\"`);\n",
    "        } else if (dataType === 'BOOLEAN') {\n",
    "            booleanColumns.push(`\"${colName}\"`);\n",
    "        } else if (['VARCHAR', 'CHAR', 'TEXT', 'STRING'].includes(dataType)) {\n",
    "            stringColumns.push(`\"${colName}\"`);\n",
    "        } else if (dataType === 'ARRAY') {\n",
    "            arrayColumns.push(`\"${colName}\"`);\n",
    "        } else if (['VARIANT', 'OBJECT'].includes(dataType)) {\n",
    "            variantObjectColumns.push(`\"${colName}\"`);\n",
    "        } else if (dataType === 'GEOGRAPHY') {\n",
    "            geographyColumns.push(`\"${colName}\"`);\n",
    "        } else if (dataType === 'TIME') {\n",
    "            timeColumns.push(`\"${colName}\"`);\n",
    "        } else {\n",
    "            otherColumns.push(`\"${colName}\"`);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Construct the final select query with the desired column order\n",
    "    var combinedColumns = columnPairs.concat(numberColumns, binaryColumns, booleanColumns, stringColumns, arrayColumns, variantObjectColumns, geographyColumns, timeColumns, otherColumns).join(', ');\n",
    "    \n",
    "    // Debug: Print the combinedColumns to check for errors\n",
    "    console.log(combinedColumns); \n",
    "    \n",
    "    var selectQuery = `CREATE OR REPLACE TABLE \"ANALYTICS\".\"BI_ENROLL_RATE_FULL_2\" AS SELECT ${combinedColumns} FROM \"EDW_PROD\".\"ANALYTICS\".\"BI_ENROLL_RATE_PROD_DATA\";`;\n",
    "\n",
    "    // Debug: Print the selectQuery to check for errors\n",
    "    console.log(selectQuery); \n",
    "    \n",
    "    try {\n",
    "        snowflake.execute({sqlText: selectQuery});\n",
    "        return 'Table \"ANALYTICS\".\"BI_ENROLL_RATE_FULL_2\" created successfully with columns in the desired order.';\n",
    "    } catch (err) {\n",
    "        return `Error when attempting to create table \"ANALYTICS\".\"BI_ENROLL_RATE_FULL_2\":\n",
    "                Query: ${selectQuery}\n",
    "                Query results: ${JSON.stringify(resultSet).substring(0, 200)}\n",
    "                combinedColumns: ${combinedColumns}\n",
    "                selectQuery: ${selectQuery}\n",
    "                selectQuery Error Code: ${err.code}\n",
    "                selectQuery Error Message: ${err.message}`;\n",
    "    }\n",
    "    $$;\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"executing sql\")\n",
    "    # Execute the command to create the Stored Procedure\n",
    "    session.sql(create_procedure_sql).collect()\n",
    "\n",
    "    print(\"calling stored procedure\")\n",
    "    # Now, you can call the newly created Stored Procedure\n",
    "    call_procedure_sql = \"CALL dynamic_date_diff_and_union_save_as_table()\"\n",
    "    result = session.sql(call_procedure_sql).collect()\n",
    "\n",
    "    print(\"result is:\")\n",
    "    # Print the result\n",
    "    print(result)\n",
    "\n",
    "#add_normalized_dates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa8f6b-8ede-4b16-95c2-aaa9c6e20969",
   "metadata": {},
   "source": [
    "#### - Add Date Sequences (For Later Date Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6557243-1009-4e3c-ac9e-078a14e3feed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#---------------- Add date sequences ----------------------\n",
    "\n",
    "from snowflake.snowpark.functions import lit, row_number\n",
    "from snowflake.snowpark.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "def add_date_sequences():\n",
    "    print(\"add_date_sequences running...\")\n",
    "    global enroll_stable_copy\n",
    "    global schema_mapped_df\n",
    "\n",
    "    enroll_stable_copy = session.table(enroll_data_table_name)\n",
    "\n",
    "    # Step 0: Add an artificial row_id for tracking rows\n",
    "    enroll_stable_copy_with_id = enroll_stable_copy.withColumn(\"row_id\", row_number().over(Window.orderBy(lit(1))))\n",
    "    \n",
    "    # Step 1: Identify date columns\n",
    "    date_columns_query = f\"\"\"\n",
    "    SELECT COLUMN_NAME\n",
    "    FROM INFORMATION_SCHEMA.COLUMNS\n",
    "    WHERE TABLE_NAME = 'BI_ENROLL_RATE_FULL_2'\n",
    "    AND DATA_TYPE IN('DATE', 'TIMESTAMP', 'TIMESTAMP_LTZ', 'TIMESTAMP_NTZ', 'TIMESTAMP_TZ');\n",
    "    \"\"\"\n",
    "    date_columns_df = session.sql(date_columns_query).collect()\n",
    "    date_column_names = [row['COLUMN_NAME'] for row in date_columns_df]\n",
    "    print(\"date_column_names\")\n",
    "    print(date_column_names)\n",
    "\n",
    "    \n",
    "    # Include date columns and normalized counterparts\n",
    "    date_columns_with_normalized = [\n",
    "        col for col in date_column_names if col in enroll_stable_copy_with_id.columns\n",
    "    ] + [\n",
    "        col + \"_NORMALIZED\" for col in date_column_names if col + \"_NORMALIZED\" in enroll_stable_copy_with_id.columns\n",
    "    ]\n",
    "\n",
    "    # Select columns including 'row_id'\n",
    "    columns_to_select = [\"row_id\"] + date_columns_with_normalized\n",
    "    enroll_stable_copy_with_dates = enroll_stable_copy_with_id.select(columns_to_select)\n",
    "    \n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    pandas_df_with_dates = enroll_stable_copy_with_dates.to_pandas()\n",
    "\n",
    "    # Convert date columns to datetime format\n",
    "    for col_name in date_column_names:\n",
    "        pandas_df_with_dates[col_name] = pd.to_datetime(pandas_df_with_dates[col_name], errors='coerce')\n",
    "\n",
    "    # Transform each row to create ordered lists\n",
    "    def transform_row_to_ordered_lists(row):\n",
    "        items = sorted(\n",
    "            [(col_name, row[col_name], row.get(col_name + \"_NORMALIZED\")) for col_name in date_column_names if pd.notnull(row[col_name])],\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "        return {\n",
    "            'ORDERED_DATE_COL_NAMES': [item[0] for item in items],\n",
    "            'ORDERED_DATES': [item[1] for item in items],\n",
    "            'ORDERED_NORMALIZED_NUMS': [item[2] for item in items],\n",
    "        }\n",
    "\n",
    "    transformed_data = pandas_df_with_dates.apply(transform_row_to_ordered_lists, axis=1)\n",
    "    pandas_df_with_dates = pd.concat([pandas_df_with_dates, pd.json_normalize(transformed_data)], axis=1)\n",
    "\n",
    "    # Drop original date columns from pandas DataFrame before converting back\n",
    "    pandas_df_final = pandas_df_with_dates.drop(columns=date_columns_with_normalized)\n",
    "\n",
    "    # Convert modified pandas DataFrame back to Snowpark DataFrame\n",
    "    modified_snowpark_df = session.create_dataframe(pandas_df_final)\n",
    "\n",
    "    # Merge modified data back using 'row_id' and update the global enroll_stable_copy\n",
    "    enroll_stable_copy = enroll_stable_copy_with_id.join(\n",
    "        modified_snowpark_df,\n",
    "        \"row_id\"\n",
    "    ).drop(\"row_id\")\n",
    "\n",
    "    display(enroll_stable_copy.to_pandas().head())\n",
    "    \n",
    "    print(\"done, pushing to snowflake\")\n",
    "    enroll_stable_copy.write.save_as_table(raw_enroll_data_table_name+\"_temporary\", mode=\"overwrite\")\n",
    "    print(\"pushed to snowflake\")\n",
    "\n",
    "#add_date_sequences()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c5279-ff51-42cf-9b1e-8cb341f1c69b",
   "metadata": {},
   "source": [
    "#### - Load Raw Source Data from Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08afaf1-80a8-484b-b734-9c4574d10f5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------Load Data-------------------------------------\n",
    "\n",
    "#deprecated... columns_to_load = [\"IS_CURRENT\"]\n",
    "def load_data(columns_to_load = None, num_rows = None):\n",
    "    #__Needed from previous functions__\n",
    "    #...\n",
    "    #--get_user_inputs--\n",
    "    global reload_data_boolean\n",
    "    global enroll_data_table_name\n",
    "    \n",
    "    #--get_or_create_session--\n",
    "    global session\n",
    "\n",
    "    #__Set for the first time in this function__\n",
    "    global enroll_stable_copy\n",
    "    global enroll_stable_copy_schema \n",
    "    global dataframe_initialized\n",
    "    global dtype_to_class_name\n",
    "\n",
    "    #___function logic starts here___\n",
    "    \n",
    "    if not dataframe_initialized or reload_data_boolean == 'Yes':\n",
    "        #number_of_rows = 3\n",
    "        # deprecated... Select specific columns and limit the number of rows (.select(columns_to_load) #.limit(number_of_rows))\n",
    "        if num_rows is not None and columns_to_load is None:\n",
    "            enroll_stable_copy = session.table(enroll_data_table_name).limit(num_rows)\n",
    "            \n",
    "        elif num_rows is None and columns_to_load is not None:\n",
    "            enroll_stable_copy = session.table(enroll_data_table_name).limit(num_rows)\n",
    "            \n",
    "        elif num_rows is not None and columns_to_load is not None:\n",
    "            enroll_stable_copy = session.table(enroll_data_table_name).select(columns_to_load).limit(num_rows)\n",
    "\n",
    "        else:\n",
    "            enroll_stable_copy = session.table(enroll_data_table_name)\n",
    "\n",
    "        \n",
    "        enroll_stable_copy_schema = enroll_stable_copy.schema\n",
    "\n",
    "        num_rows = enroll_stable_copy.count()\n",
    "        print(f\"Number of rows: {num_rows}\")\n",
    "        \n",
    "        num_columns = len(enroll_stable_copy_schema.fields)\n",
    "        print(f\"Number of columns: {num_columns}\")\n",
    "\n",
    "        dataframe_initialized = True \n",
    "\n",
    "        dtype_to_class_name = {}\n",
    "        for field in enroll_stable_copy_schema.fields:\n",
    "            # key = str of  datatype\n",
    "            # value = class name\n",
    "            dtype_to_class_name[str(field.datatype)] = field.datatype.__class__.__name__\n",
    "\n",
    "#load_data()\n",
    " # Update flag to indicate dataframe is now initialized\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e2ffab-613a-4ab5-93df-fc31dc666c99",
   "metadata": {},
   "source": [
    "#### - Map Native Data Types -> Simplified Test Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7617cbb8-3da1-41d5-ba56-638c1535fa9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------Map Native Data Types => Test Data Types-------------------------------------\n",
    "def map_data_types_to_categories():\n",
    "    #__Needed from previous functions__\n",
    "    #--get_or_create_session--\n",
    "    global session\n",
    "    \n",
    "    #--load_data--\n",
    "    global enroll_stable_copy\n",
    "    global enroll_stable_copy_schema\n",
    "    global dtype_to_class_name\n",
    "\n",
    "    #__Set for the first time in this function__\n",
    "    global schema_mapped_df\n",
    "    \n",
    "    #___function logic starts here___\n",
    "    \n",
    "    Category_Mapping_Reference = {\n",
    "        'STRING': ['StringType'],\n",
    "        'BINARY': ['BinaryType', 'BooleanType'],\n",
    "        'NUMBER': ['DecimalType', 'DoubleType', 'FloatType', 'IntegerType', 'LongType', 'ShortType'],\n",
    "        'DATE': ['DateType', 'TimestampType'],\n",
    "        'TIME': ['TimeType'],\n",
    "        'VARIANT': ['ArrayType', 'MapType', 'Variant', 'VariantType'],\n",
    "        'GEOGRAPHY': ['Geography', 'GeographyType'],\n",
    "        'OTHER': ['ByteType', 'ColumnIdentifier', 'DataType', 'StructField', 'StructType']\n",
    "    }\n",
    "    \n",
    "    # Function to create DataFrame from mappings\n",
    "    def create_dtype_mapping_df(dtype_to_class, category_mapping):\n",
    "        data = []\n",
    "        for dtype, class_name in dtype_to_class.items():\n",
    "            # Find the category for the class name\n",
    "            category = 'OTHER'\n",
    "            for cat, types in category_mapping.items():\n",
    "                if class_name in types:\n",
    "                    category = cat\n",
    "                    break\n",
    "    \n",
    "            data.append({\n",
    "                'native_data_type': dtype,\n",
    "                'native_data_class': class_name,\n",
    "                'test_data_type': category\n",
    "            })\n",
    "    \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def apply_mapping_to_schema(schema, mapping_df):\n",
    "        # Prepare data for the DataFrame\n",
    "        data = []\n",
    "        for field in schema.fields:\n",
    "            dtype_str = str(field.datatype)\n",
    "            # Find the mapping in the DataFrame\n",
    "            mapping = mapping_df[mapping_df['native_data_type'] == dtype_str].iloc[0]\n",
    "            \n",
    "            data.append({\n",
    "                'column_name': field.name,\n",
    "                'native_data_type': mapping['native_data_type'],\n",
    "                'native_data_class': mapping['native_data_class'],\n",
    "                'test_data_type': mapping['test_data_type']\n",
    "            })\n",
    "    \n",
    "        # Create and return the DataFrame\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #-----------background/context-----\n",
    "    #print(enroll_stable_copy_schema)\n",
    "    #print(\"Revant previous steps/variables so far:\")\n",
    "    #print(\"enroll_stable_copy = session.table(enroll_data_table_name)#.select(columns_to_load) #.limit(number_of_rows)\")\n",
    "    #print(\"enroll_stable_copy_schema = enroll_stable_copy.schema\")\n",
    "    #print(\"enroll_stable_copy_schema:\")\n",
    "    #print(\"display(dtype_to_class_name)\")\n",
    "    #display(dtype_to_class_name)\n",
    "    \n",
    "    \n",
    "    #---------function call/printing-------\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    dtype_mapping_df = create_dtype_mapping_df(dtype_to_class_name, Category_Mapping_Reference)\n",
    "    \n",
    "    \n",
    "    # Assuming enroll_stable_copy_schema and dtype_mapping_df are defined\n",
    "    schema_mapped_df = apply_mapping_to_schema(enroll_stable_copy_schema, dtype_mapping_df)\n",
    "    \n",
    "    print(\"upper-casing pandas columns before transform\")\n",
    "    schema_mapped_df.columns = schema_mapped_df.columns.str.upper()\n",
    "    print(\"\")\n",
    "    print(\"Mapping is:\")\n",
    "    #pd.options.display.max_rows = None\n",
    "    showall(schema_mapped_df)\n",
    "    #showall(schema_mapped_df[schema_mapped_df['TEST_DATA_TYPE'] == 'DATE'])\n",
    "    #pd.options.display.max_rows = 60\n",
    "\n",
    "\n",
    "#map_data_types_to_categories()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d65990c9-3ffb-443a-a68c-f07328cbf08c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#---------------- Add date sequences (old, reference) ----------------------\n",
    "\n",
    "import pandas as pd\n",
    "from snowflake.snowpark.functions import lit, row_number\n",
    "from snowflake.snowpark import Session, DataFrame\n",
    "from snowflake.snowpark.window import Window\n",
    "\n",
    "# Assuming 'session' is an active Snowpark Session and 'schema_mapped_df' is your schema DataFrame\n",
    "\n",
    "def add_date_sequences (enroll_stable_copy: DataFrame, schema_mapped_df: pd.DataFrame) -> DataFrame:\n",
    "    # Step 0: Add an artificial row_id to enroll_stable_copy for tracking rows\n",
    "    enroll_stable_copy_with_id = enroll_stable_copy.withColumn(\"row_id\", row_number().over(Window.orderBy(lit(1))))\n",
    "\n",
    "    # Step 1: Dynamically identify date columns from the schema information\n",
    "    date_column_names = schema_mapped_df[schema_mapped_df['TEST_DATA_TYPE'] == 'DATE']['COLUMN_NAME'].tolist()\n",
    "\n",
    "    # Including both date columns and their normalized counterparts, if they exist\n",
    "    date_columns_with_normalized = [\n",
    "        col for col in date_column_names if col in enroll_stable_copy_with_id.columns\n",
    "    ] + [\n",
    "        col + \"_NORMALIZED\" for col in date_column_names if col + \"_NORMALIZED\" in enroll_stable_copy_with_id.columns\n",
    "    ]\n",
    "\n",
    "    # Step 2: Select required columns, ensuring to include the 'row_id'\n",
    "    columns_to_select = [\"row_id\"] + date_columns_with_normalized\n",
    "    enroll_stable_copy_with_dates = enroll_stable_copy_with_id.select(columns_to_select)\n",
    "\n",
    "    # Convert to a pandas DataFrame for further processing\n",
    "    pandas_df_with_dates = enroll_stable_copy_with_dates.to_pandas()\n",
    "\n",
    "    # Convert date columns to datetime format and prepare for dict creation\n",
    "    for col_name in date_column_names:\n",
    "        pandas_df_with_dates[col_name] = pd.to_datetime(pandas_df_with_dates[col_name], errors='coerce')\n",
    "\n",
    "    # Transform each row to create separate columns for ordered date names, dates, and normalized numbers\n",
    "    def transform_row_to_ordered_lists(row):\n",
    "        items = sorted(\n",
    "            [(col_name, row[col_name], row.get(col_name + \"_NORMALIZED\")) for col_name in date_column_names if pd.notnull(row[col_name])],\n",
    "            key=lambda x: x[1]  # Sorting by date\n",
    "        )\n",
    "        return {\n",
    "            'ORDERED_DATE_COL_NAMES': [item[0] for item in items],\n",
    "            'ORDERED_DATES': [item[1] for item in items],\n",
    "            'ORDERED_NORMALIZED_NUMS': [item[2] for item in items],\n",
    "        }\n",
    "\n",
    "    # Apply the transformation to each row\n",
    "    transformed_data = pandas_df_with_dates.apply(transform_row_to_ordered_lists, axis=1)\n",
    "    for column in ['ORDERED_DATE_COL_NAMES', 'ORDERED_DATES', 'ORDERED_NORMALIZED_NUMS']:\n",
    "        pandas_df_with_dates[column] = transformed_data.apply(lambda x: x[column])\n",
    "\n",
    "    # Convert the modified pandas DataFrame back to a Snowpark DataFrame\n",
    "    modified_snowpark_df = session.create_dataframe(pandas_df_with_dates.drop(columns=date_columns_with_normalized))\n",
    "\n",
    "    # Merge modified data back into the original DataFrame using the artificial 'row_id'\n",
    "\n",
    "    merged_df = enroll_stable_copy_with_id.join(\n",
    "        modified_snowpark_df,\n",
    "        enroll_stable_copy_with_id[\"row_id\"] == modified_snowpark_df[\"row_id\"]\n",
    "    ).drop(modified_snowpark_df[\"row_id\"]).drop(\"row_id\")  # Added .drop(\"row_id\") to remove the artificial row_id\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14203e-2e5a-4507-b20c-66b3e303bc87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Column Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e4b2a6-ab84-4d8e-926d-f7ae84937af9",
   "metadata": {},
   "source": [
    "#### - Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0dc2f2-2743-4fdf-9f98-0b7b83f42af3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#------------------------Run Column Analysis-------------------------------------\n",
    "\n",
    "def run_column_analysis (subset = None):\n",
    "    #__Needed from previous functions__\n",
    "    #--get_or_create_session--\n",
    "    global session\n",
    "    #--load_data--\n",
    "    global enroll_stable_copy\n",
    "\n",
    "    #--map_data_types_to_categories--\n",
    "    global schema_mapped_df\n",
    "\n",
    "    #__Set for the first time in this function__\n",
    "    global test_results_df\n",
    "\n",
    "    #___function logic starts here___\n",
    "    if subset == None:\n",
    "        print(\"No subset provided - using original raw snowflake data\")\n",
    "        raw_data_subset = enroll_stable_copy\n",
    "    else:\n",
    "        print(\"Subset provided - using subset instead of original raw snowflake data\")\n",
    "        raw_data_subset = subset\n",
    "\n",
    "    easy_tests = {\n",
    "        'GENERAL': ['NULL_PERCENTAGE','DISTINCT_VALUES'],\n",
    "        'STRING': ['STRING_AVG_LENGTH','STRING_MAX_LENGTH','STRING_MIN_LENGTH'],\n",
    "        'NUMBER': ['NUMBER_MIN','NUMBER_AVG','NUMBER_MEDIAN','NUMBER_MODE','NUMBER_MAX','NUMBER_STD_DEV','NUMBER_LOWER_QUARTILE','NUMBER_UPPER_QUARTILE'],\n",
    "        'BINARY': ['BINARY_PERCENT_TRUE_OR_1','BINARY_PERCENT_FALSE_OR_0'],\n",
    "        'DATE': ['DATE_MIN','DATE_MAX']\n",
    "    }\n",
    "    \n",
    "    hard_tests = {\n",
    "        'VARIANT': ['VARIANT_UNIQUE_KEYS','VARIANT_AVG_VALUE_LENGTH','VARIANT_UNIQUE_VALUE_TYPES'],\n",
    "        'CATEGORICAL': ['CATEGORICAL_UNIQUE_CATEGORIES_COUNT','CATEGORICAL_UNIQUE_CATEGORIES_LIST_TOP_50','CATEGORICAL_CATEGORY_FREQUENCY','CATEGORICAL_PERCENTAGE_OF_TOTAL_PER_CATEGORY','CATEGORICAL_DISTRIBUTION_NORMALIZATION_COEF','CATEGORICAL_DISTRIBUTION_SKEW_COEF'],\n",
    "        'STRING': ['STRING_COMMON_PATTERNS'],\n",
    "        'NUMBER': ['NUMBER_DISTRIBUTION_NORMALIZATION_COEF','NUMBER_DISTRIBUTION_SKEW_COEF'],\n",
    "        'DATE': ['DATE_AVG','DATE_FREQUENCY_DISTRIBUTION'],\n",
    "        'GEOGRAPHY': ['GEOGRAPHY_DATA_TYPES']#find more?\n",
    "    }\n",
    "    \n",
    "    test_column_subsets = {}\n",
    "\n",
    "    print(\"----------starting testing----------\")\n",
    "    \"\"\" Pulls only needed data for specific tests from a large dataset. \"\"\"\n",
    "\n",
    "    print(\"Copy schema_mapped_df with COLUMN_NAME as index for results compilation\")\n",
    "    test_results_df = schema_mapped_df.set_index(schema_mapped_df.columns[0])\n",
    "    \n",
    "    #combined_keys = list(set(easy_tests.keys()) | set(hard_tests.keys()))\n",
    "    \n",
    "    combined_keys = ['GENERAL','DATE','NUMBER','BINARY','STRING','VARIANT','GEOGRAPHY','CATEGORICAL']\n",
    "    #combined_keys = ['DATE']\n",
    "    \n",
    "    for test_data_type in combined_keys:\n",
    "        subset = schema_mapped_df.loc[schema_mapped_df['TEST_DATA_TYPE'] == test_data_type]\n",
    "        #print(\"filter_data_and_feed_tests 0: for test_data_type in easy_tests.keys():\")\n",
    "        #print(subset)\n",
    "        if subset.empty:\n",
    "            if test_data_type == 'GENERAL':\n",
    "                #print(\"filter_data_and_feed_tests 1: subset empty, setting GENERAL subset to all\")\n",
    "                test_column_subsets[test_data_type] = schema_mapped_df\n",
    "            else:\n",
    "                #print(\"filter_data_and_feed_tests 2: Not Found\")\n",
    "                print(f\"No columns for test_data_type '{test_data_type}' found.\")\n",
    "        else:\n",
    "            #print(\"filter_data_and_feed_tests 3: Subsetting data\")\n",
    "            test_column_subsets[test_data_type] = subset\n",
    "            #print(\"filter_data_and_feed_tests 4: DATA SUBSET IS:\")\n",
    "            #print(test_column_subsets[test_data_type])\n",
    "\n",
    "    \n",
    "    for test_data_type in test_column_subsets.keys():\n",
    "        #print(\"filter_data_and_feed_tests 5: For loop, current data type is:\")\n",
    "        print(\"Testing data type: \",test_data_type)\n",
    "        \n",
    "        if test_data_type == 'GENERAL':\n",
    "            test_columns = test_column_subsets[test_data_type]['COLUMN_NAME'].tolist()\n",
    "            print(\"raw_data_subset:\")\n",
    "            test_data = raw_data_subset.select(test_columns)\n",
    "            for test in easy_tests[test_data_type]:\n",
    "                if test == 'NULL_PERCENTAGE':#---------------------------------\n",
    "                    print('   NULL_PERCENTAGE...')\n",
    "                    # Create an aggregate expression for each column to calculate the percentage of NULLs\n",
    "                    NULL_PERCENTAGE_expression = [\n",
    "                        ((count(lit(1)) - count(col(name))) / count(lit(1)) * 100).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    percent_null_df = test_data.agg(*NULL_PERCENTAGE_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(percent_null_df, how='left')\n",
    "                    \n",
    "                if test == 'DISTINCT_VALUES': #get distinct values AND # of distinct\n",
    "                    #--DISTINCT_VALUES--------------------------------------------\n",
    "                    print('   DISTINCT_VALUES...')\n",
    "                                      \n",
    "                    DISTINCT_VALUES_expression = [\n",
    "                        collect_set(col(name)).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    DISTINCT_VALUES_df = test_data.agg(*DISTINCT_VALUES_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    \n",
    "                    DISTINCT_VALUES_df = DISTINCT_VALUES_df.replace('\\n\\s*|\\n$', '', regex=True)\n",
    "\n",
    "\n",
    "                    def try_convert_to_list(x):\n",
    "                        try:\n",
    "                            # Correct boolean values to Python's True and False\n",
    "                            corrected_x = x.replace('true', 'True').replace('false', 'False')\n",
    "                            # Attempt to evaluate the corrected string as a Python literal\n",
    "                            evaluated = ast.literal_eval(corrected_x)\n",
    "                            # Ensure the evaluated result is a list\n",
    "                            if isinstance(evaluated, list):\n",
    "                                return evaluated\n",
    "                            return []\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error with value: {x}. Error: {e}\")\n",
    "                            return []\n",
    "                          \n",
    "                    DISTINCT_VALUES_df = DISTINCT_VALUES_df.applymap(try_convert_to_list)\n",
    "                    \n",
    "                    test_results_df = test_results_df.join(DISTINCT_VALUES_df.applymap(json.dumps), how='left')\n",
    "                    \n",
    "                    #--DISTINCT_COUNT--------------------------------------------\n",
    "                    DISTINCT_COUNT_df = DISTINCT_VALUES_df.applymap(len).rename(columns={test:\"DISTINCT_COUNT\"})\n",
    "    \n",
    "                    test_results_df = test_results_df.join(DISTINCT_COUNT_df, how='left')\n",
    "\n",
    "                    print(\"distinct done.\")        \n",
    "                    \n",
    "\n",
    "        elif test_data_type == 'BINARY':\n",
    "            test_columns = test_column_subsets[test_data_type]['COLUMN_NAME'].tolist()\n",
    "            test_data = raw_data_subset.select(test_columns)\n",
    "            for test in easy_tests[test_data_type]:\n",
    "                if test == 'BINARY_PERCENT_TRUE_OR_1':#---------------------------------\n",
    "                    print('   BINARY_PERCENT_TRUE_OR_1...')\n",
    "                    \n",
    "                    PERCENT_TRUE_OR_1_expression = [\n",
    "                        ((count(when(col(name).isin([1, True]), 1)) / count(lit(1)) * 100).alias(name))\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    PERCENT_TRUE_OR_1_df = test_data.agg(*PERCENT_TRUE_OR_1_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(PERCENT_TRUE_OR_1_df, how='left')\n",
    "                    \n",
    "                if test == 'BINARY_PERCENT_FALSE_OR_0':#---------------------------------\n",
    "                    print('   BINARY_PERCENT_FALSE_OR_0...')\n",
    "                    \n",
    "                    PERCENT_FALSE_OR_0_expression = [\n",
    "                        ((count(when(col(name).isin([0, False]), True)) / count(lit(1)) * 100).alias(name))\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    PERCENT_FALSE_OR_0_df = test_data.agg(*PERCENT_FALSE_OR_0_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(PERCENT_FALSE_OR_0_df, how='left')\n",
    "       \n",
    "        # elif test_data_type != '':\n",
    "       #     test_data_type = 'fdsafdsafsd'\n",
    "        \n",
    "        elif test_data_type == 'STRING':\n",
    "            test_columns = test_column_subsets[test_data_type]['COLUMN_NAME'].tolist()\n",
    "            test_data = raw_data_subset.select(test_columns)\n",
    "            for test in easy_tests[test_data_type]:\n",
    "                if test == 'STRING_MAX_LENGTH':#---------------------------------\n",
    "                    print('   STRING_MAX_LENGTH...')\n",
    "                    \n",
    "                    MAX_LENGTH_expression = [\n",
    "                        (max(length(col(name)))).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    MAX_LENGTH_df = test_data.agg(*MAX_LENGTH_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(MAX_LENGTH_df, how='left')\n",
    "                    \n",
    "                if test == 'STRING_AVG_LENGTH':#---------------------------------\n",
    "                    print('   STRING_AVG_LENGTH...')\n",
    "                    \n",
    "                    AVG_LENGTH_expression = [\n",
    "                        (avg(length(col(name)))).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    AVG_LENGTH_df = test_data.agg(*AVG_LENGTH_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(AVG_LENGTH_df, how='left')\n",
    "                    \n",
    "                if test == 'STRING_MIN_LENGTH':#---------------------------------\n",
    "                    print('   STRING_MIN_LENGTH...')\n",
    "                    \n",
    "                    MIN_LENGTH_expression = [\n",
    "                        (min(length(col(name)))).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    MIN_LENGTH_df = test_data.agg(*MIN_LENGTH_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(MIN_LENGTH_df, how='left')\n",
    "                    \n",
    "        elif test_data_type == 'NUMBER':\n",
    "            \n",
    "            test_columns = test_column_subsets[test_data_type]['COLUMN_NAME'].tolist()\n",
    "            test_data = raw_data_subset.select(test_columns)\n",
    "            for test in easy_tests[test_data_type]:\n",
    "                if test == 'NUMBER_MIN':#---------------------------------\n",
    "                    print('   NUMBER_MIN...')\n",
    "                    \n",
    "                    MIN_expression = [\n",
    "                        (min(col(name))).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    MIN_df = test_data.agg(*MIN_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(MIN_df, how='left')\n",
    "                    \n",
    "                if test == 'NUMBER_AVG':#---------------------------------\n",
    "                    print('   NUMBER_AVG...')\n",
    "                    \n",
    "                    AVG_expression = [\n",
    "                        (avg(col(name))).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    AVG_df = test_data.agg(*AVG_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(AVG_df, how='left')\n",
    "                    \n",
    "                if test == 'NUMBER_MEDIAN':#---------------------------------\n",
    "                    print('   NUMBER_MEDIAN...')\n",
    "                    \n",
    "                    MEDIAN_expression = [\n",
    "                        (approx_percentile(name, 0.5)).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    MEDIAN_df = test_data.agg(*MEDIAN_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(MEDIAN_df, how='left')\n",
    "                    \n",
    "                if test == 'NUMBER_MODE':#---------------------------------\n",
    "                    print('   NUMBER_MODE...')\n",
    "                    \n",
    "                    MODE_expression = [\n",
    "                        (mode(col(name))).alias(name)  # Assuming 'mode' is a predefined UDF for calculating the mode of a column\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    MODE_df = test_data.agg(*MODE_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(MODE_df, how='left')\n",
    "\n",
    "                if test == 'NUMBER_MAX':#---------------------------------\n",
    "                    print('   NUMBER_MAX...')\n",
    "                    \n",
    "                    MAX_expression = [\n",
    "                        (max(col(name))).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    MAX_df = test_data.agg(*MAX_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(MAX_df, how='left')\n",
    "                    \n",
    "                if test == 'NUMBER_STD_DEV':#---------------------------------\n",
    "                    print('   NUMBER_STD_DEV...')\n",
    "                    \n",
    "                    STD_DEV_expression = [\n",
    "                        (stddev(col(name))).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    STD_DEV_df = test_data.agg(*STD_DEV_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(STD_DEV_df, how='left')\n",
    "                    \n",
    "                if test == 'NUMBER_LOWER_QUARTILE':#---------------------------------\n",
    "                    print('   NUMBER_LOWER_QUARTILE...')\n",
    "                    \n",
    "                    LOWER_QUARTILE_expression = [\n",
    "                        (approx_percentile(col(name), 0.25)).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    LOWER_QUARTILE_df = test_data.agg(*LOWER_QUARTILE_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(LOWER_QUARTILE_df, how='left')\n",
    "                    \n",
    "                if test == 'NUMBER_UPPER_QUARTILE':#---------------------------------\n",
    "                    print('   NUMBER_UPPER_QUARTILE...')\n",
    "                    \n",
    "                    UPPER_QUARTILE_expression = [\n",
    "                        (approx_percentile(col(name), 0.75)).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    UPPER_QUARTILE_df = test_data.agg(*UPPER_QUARTILE_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(UPPER_QUARTILE_df, how='left')\n",
    "\n",
    "        elif test_data_type == 'DATE':\n",
    "            test_columns = test_column_subsets[test_data_type]['COLUMN_NAME'].tolist()\n",
    "            test_data = raw_data_subset.select(test_columns)\n",
    "            for test in easy_tests[test_data_type]:\n",
    "                if test == 'DATE_MIN':#---------------------------------\n",
    "                    print('   DATE_MIN...')\n",
    "                    \n",
    "                    MIN_expression = [\n",
    "                        (min(col(name))).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    MIN_df = test_data.agg(*MIN_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(MIN_df, how='left')\n",
    "                    \n",
    "                if test == 'DATE_MAX':#---------------------------------\n",
    "                    print('   DATE_MAX...')\n",
    "                    \n",
    "                    MAX_expression = [\n",
    "                        (max(col(name))).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    MAX_df = test_data.agg(*MAX_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(MAX_df, how='left')\n",
    "                    \n",
    "            for test in hard_tests[test_data_type]:\n",
    "                if test == 'DATE_AVG':#---------------------------------\n",
    "                    print('   DATE_AVG...')\n",
    "                    \n",
    "                    AVG_expression = [\n",
    "                        to_date(\n",
    "                        from_unixtime(\n",
    "                            avg(unix_timestamp(col(name))\n",
    "                               ))).alias(name)\n",
    "                        for name in test_columns\n",
    "                    ]\n",
    "                    \n",
    "                    AVG_df = test_data.agg(*AVG_expression).to_pandas().transpose().rename(columns={0:test})\n",
    "                    test_results_df = test_results_df.join(AVG_df, how='left')\n",
    "        else:\n",
    "            print(\"No tests for data type:\",test_data_type)\n",
    "\n",
    "    # Convert the index into the first column\n",
    "    test_results_df = test_results_df.reset_index()\n",
    "\n",
    "    display(test_results_df)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return(test_results_df)\n",
    "\n",
    "    \n",
    "#run_column_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970100e-c798-4269-91c7-f77496de947c",
   "metadata": {},
   "source": [
    "#### - Push Analysis to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6d078ee-7a10-4f4a-ab32-b2e24506b35e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------Push Column Analysis to Snowflake-------------------------------------\n",
    "\n",
    "def upload_column_analysis_to_snowflake ():\n",
    "    #__Needed from previous functions__\n",
    "    #--get_user_inputs--\n",
    "    global col_analysis_table_name\n",
    "    #--get_or_create_session--\n",
    "    global session\n",
    "    #--run_column_tests--\n",
    "    global test_results_df\n",
    "    \n",
    "    #__Set for the first time in this function__\n",
    "    global column_analysis\n",
    "    global snowflake_updated_column_analysis\n",
    "    \n",
    "    #___function logic starts here___\n",
    "    \n",
    "    #print(test_results_df.columns)\n",
    "    #print(int(time()))\n",
    "    print(\"#-----get data from before merge-----\")\n",
    "    print(\"\")\n",
    "    #print(\"running before_sql_query...\")\n",
    "    \n",
    "    before_sql_query = \"SELECT * FROM ENROLL_RATE_COLUMN_ANALYSIS\"\n",
    "    before_pandas_df = session.sql(before_sql_query).to_pandas()\n",
    "    before_pandas_df = before_pandas_df.reindex(columns=test_results_df.columns)\n",
    "    \n",
    "    #print(\"#-----write test_results_df to a temporary table-----\")\n",
    "    temp_table_name = 'temp_test_results'\n",
    "    \n",
    "    print(\"-----dropping old temp table, and making new-----\")\n",
    "    print(\"\")\n",
    "    \n",
    "    drop_table_sql = f\"\"\"\n",
    "    DROP TABLE IF EXISTS \"{temp_table_name}\";\n",
    "    \"\"\"\n",
    "    session.sql(drop_table_sql).collect()\n",
    "    \n",
    "    session.write_pandas(test_results_df, temp_table_name, auto_create_table=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_snowflake_table_columns(session: Session, table_name: str) -> list:\n",
    "        get_columns_SQL = f'DESC TABLE \"EDW_PROD\".\"ANALYTICS\".\"{table_name}\"'\n",
    "        result = session.sql(get_columns_SQL).collect()\n",
    "        target_columns = [row[\"name\"].upper() for row in result]\n",
    "    \n",
    "        return target_columns\n",
    "    \n",
    "    #print(\"Temp Table Columns Are...\")\n",
    "    testttttt = get_snowflake_table_columns(session, temp_table_name)\n",
    "    #print(testttttt)\n",
    "    #print(\"\")\n",
    "    #print(\"\")\n",
    "    #print(\"\")\n",
    "    \n",
    "    \n",
    "    print(\"#-----add missing columns-----\")\n",
    "    print(\"\")\n",
    "    # Define the target Snowflake table name\n",
    "    target_table_name = col_analysis_table_name\n",
    "    \n",
    "    #TEMP STEP!!! <- Remove everything except 'COLUMN_NAME', 'NATIVE_DATA_TYPE', 'TEST_DATA_TYPE', 'NATIVE_DATA_CLASS', 'FUNCTIONAL_CATEGORY'\n",
    "    keep_list = ['COLUMN_NAME', 'NATIVE_DATA_TYPE', 'TEST_DATA_TYPE', 'NATIVE_DATA_CLASS', 'FUNCTIONAL_CATEGORY']\n",
    "    columns_to_reset = [i for i in get_snowflake_table_columns(session, target_table_name) if i not in keep_list]\n",
    "    \n",
    "    #print(\"columns to remove are:\")\n",
    "    #print(columns_to_reset)\n",
    "    #print(\"\")\n",
    "    \n",
    "    if len(columns_to_reset) != 0:\n",
    "        # Step 4: For each missing column, construct and execute an ALTER TABLE statement to add the column\n",
    "        for column in columns_to_reset:\n",
    "            alter_table_sql = f\"ALTER TABLE {target_table_name} DROP COLUMN {column};\"\n",
    "            session.sql(alter_table_sql).collect()\n",
    "    \n",
    "    # Step 1: Get the columns from the pandas DataFrame (source schema)\n",
    "    source_columns = test_results_df.columns.tolist()\n",
    "    #print(\"source_columns:\")\n",
    "    #print(source_columns)\n",
    "    #print(\"\")\n",
    "    \n",
    "    # Step 2: Get the columns from the Snowflake table (target schema)\n",
    "    #print(\"target_columns are...\")\n",
    "    target_columns = get_snowflake_table_columns(session, target_table_name)\n",
    "    #print(target_columns)\n",
    "    #print(\"\")\n",
    "    \n",
    "    # Step 3: Identify missing columns in the target schema\n",
    "    missing_columns = [element for element in source_columns if element not in target_columns]\n",
    "    #print(\"Columns to Add:\")\n",
    "    #print(missing_columns)\n",
    "    #print(\"\")\n",
    "    \n",
    "    if len(missing_columns) != 0:\n",
    "        # Step 4: For each missing column, construct and execute an ALTER TABLE statement to add the column\n",
    "        for column in missing_columns:\n",
    "            column_type = \"STRING\" #\"DATE\" if \"DATE\" in column.upper() else \"STRING\"\n",
    "            alter_table_sql = f\"ALTER TABLE {target_table_name} ADD COLUMN {column} {column_type};\"\n",
    "            session.sql(alter_table_sql).collect()\n",
    "    \n",
    "    #print(\"Checking for columns added...\")\n",
    "    target_columns_after_drop_and_add = get_snowflake_table_columns(session, target_table_name)\n",
    "    #print(\"columns removed:\")\n",
    "    #print(set(target_columns) - set(target_columns_after_drop_and_add))\n",
    "    #print(\"\")\n",
    "    #print(\"columns added:\")\n",
    "    #print(set(target_columns_after_drop_and_add) - set(target_columns))\n",
    "    #print(\"\")\n",
    "    #print(\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"#-----perform merge-----\")\n",
    "    print(\"\")\n",
    "    # Construct update and insert clauses dynamically\n",
    "    update_set = \",\\n\".join([f\"target.{col} = COALESCE(source.{col}, target.{col})\" for col in source_columns])\n",
    "    insert_columns = \", \".join(list(source_columns))\n",
    "    insert_values = \", \".join([f\"source.{col}\" for col in source_columns])\n",
    "    \n",
    "    # Construct the MERGE SQL statement\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO \"ENROLL_RATE_COLUMN_ANALYSIS\" AS target\n",
    "    USING \"{temp_table_name}\" AS source\n",
    "    ON target.column_name = source.column_name\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "        {update_set}\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT ({insert_columns})\n",
    "        VALUES ({insert_values});\n",
    "    \"\"\"\n",
    "    #print(\"executing SQL:\")\n",
    "    #print(merge_sql)\n",
    "    #print(\"\")\n",
    "    \n",
    "    # Execute the MERGE statement\n",
    "    session.sql(merge_sql).collect()\n",
    "    \n",
    "    print(\"#-----display what was uploaded:-----\")\n",
    "    #print(\"running after_sql_query...\")\n",
    "    print(\"\")\n",
    "    after_sql_query = \"SELECT * FROM ENROLL_RATE_COLUMN_ANALYSIS\"\n",
    "    after_pandas_df = session.sql(after_sql_query).to_pandas()\n",
    "    after_pandas_df = after_pandas_df.reindex(columns=test_results_df.columns)\n",
    "    \n",
    "    #print(\"before_pandas_df.columns:\")\n",
    "    #print(before_pandas_df.columns)\n",
    "    #print(\"\")\n",
    "    #print(\"after_pandas_df.columns:\")\n",
    "    #print(after_pandas_df.columns)\n",
    "    #print(\"\")\n",
    "    #print(\"Displaying before DF...\")\n",
    "    #display(before_pandas_df)\n",
    "    #print(\"Displaying after DF...\")\n",
    "    display(after_pandas_df)\n",
    "\n",
    "    snowflake_updated_column_analysis = after_pandas_df\n",
    "        \n",
    "    \n",
    "    print(\"#------------------------------drop temp table------------------------------\")\n",
    "    # Clean up by dropping the temporary table\n",
    "    session.sql(f\"DROP TABLE IF EXISTS {temp_table_name}\").collect()\n",
    "\n",
    "#upload_column_analysis_to_snowflake()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac51bb0-cf96-498b-9767-a2c4e3ca2a87",
   "metadata": {},
   "source": [
    "## Date Sequence Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "709ff4f2-bd49-4303-bd30-7a4883830f06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#print(test_results_df[['COLUMN_NAME','DISTINCT_VALUES']])\n",
    "test_results_df[test_results_df['COLUMN_NAME'].isin(['ORDERED_DATE_COL_NAMES','ORDERED_DATES','ORDERED_NORMALIZED_NUMS'])]\n",
    "#print(test_results_df[test_results_df['COLUMN_NAME'] == 'ORDERED_DATES']['DISTINCT_VALUES'])\n",
    "#print(test_results_df[test_results_df['COLUMN_NAME'] == 'ORDERED_NORMALIZED_NUMS']['DISTINCT_VALUES'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9240dd9-2688-4170-bd8d-68084a0d68d5",
   "metadata": {},
   "source": [
    "## MLR / Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133659dd-e3ea-4f15-ad29-73206db9b4f1",
   "metadata": {},
   "source": [
    "#### - Data Cleaning & Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4559563f-6db2-4314-a0cb-3f53dff554aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Paused - def check_cols(df):\n",
    "    # Define the columns to check for\n",
    "    columns_to_check = ['REPORT_DATE', 'IDENTIFIED_FLAG', 'DOCUMENT_COMPLETION_STATUS']\n",
    "    \n",
    "    # Initialize a result dictionary\n",
    "    result = {column: False for column in columns_to_check}\n",
    "    \n",
    "    # Check for columns based on the type of df\n",
    "    # We check for an attribute that is unique to pandas DataFrame\n",
    "    if hasattr(df, 'to_numpy'):  # 'to_numpy' method is specific to pandas DataFrames\n",
    "        # It's a pandas DataFrame\n",
    "        for column in columns_to_check:\n",
    "            result[column] = column in df.columns\n",
    "    # Else, we assume it's a Snowpark DataFrame\n",
    "    elif callable(getattr(df, 'schema', None)):  # 'schema' method is specific to Snowpark DataFrames\n",
    "        snowpark_columns = [field.name for field in df.schema.fields]\n",
    "        for column in columns_to_check:\n",
    "            result[column] = column in snowpark_columns\n",
    "    else:\n",
    "        raise TypeError(\"The input does not appear to be a pandas or Snowpark DataFrame.\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fef2c629-ceb2-492c-9d44-427e2ce5f444",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#--------------------------    Classify Columns for .Corr / ANOVA     ----------------------------\n",
    "\n",
    "def classify_columns_for_corr_and_anova():\n",
    "    #__Needed from previous functions__\n",
    "    #--load_data()--\n",
    "    global enroll_stable_copy\n",
    "    #--map_data_types_to_categories--\n",
    "    global schema_mapped_df\n",
    "    #--run_column_analysis--\n",
    "    global test_results_df\n",
    "\n",
    "    #__Set for the first time in this function__\n",
    "    global categorical_corr_anova_threshold #manually set in user input function\n",
    "    global main_target_column #placeholder, need to consolidate & replace hardcoding first. already initialized in params cell.\n",
    "    global NUMERIC_COLUMN_NAMES\n",
    "    global CATEGORICAL_COLUMN_NAMES\n",
    "    global CATEGORICAL_COLUMN_NAMES_plus_goal #placeholder, not sure if I'll need. Not currently initialized in params cell.\n",
    "    global IGNORE_COLUMN_NAMES\n",
    "    \n",
    "    #___function logic starts here___\n",
    "    threshold = categorical_corr_anova_threshold\n",
    "    \n",
    "    import pandas as pd\n",
    "    print(\"Starting...\")\n",
    "    \n",
    "    test_results_to_remove = test_results_df[\n",
    "        (test_results_df['NULL_PERCENTAGE'] == 1) |\n",
    "        ((test_results_df['NULL_PERCENTAGE'] == 0) & (test_results_df['DISTINCT_COUNT'] == 1)) |\n",
    "        (test_results_df['DISTINCT_COUNT'] == 0) |\n",
    "        (test_results_df['DISTINCT_VALUES'].isna()) #|\n",
    "        #(test_results_df['COLUMN_NAME'] == 'NON_APP_REFERRAL_DATE')\n",
    "    ]\n",
    "    test_results_to_remove = test_results_to_remove[['COLUMN_NAME','NULL_PERCENTAGE','DISTINCT_VALUES','DISTINCT_COUNT']]\n",
    "    IGNORE_COLUMN_NAMES = test_results_to_remove['COLUMN_NAME'].tolist()\n",
    "    \n",
    "    print(\"Columns removed from testing:\")\n",
    "    display(test_results_to_remove)\n",
    "    \n",
    "    filtered_test_results_df = test_results_df[\n",
    "        ~(\n",
    "            (test_results_df['NULL_PERCENTAGE'] == 1) |\n",
    "            ((test_results_df['NULL_PERCENTAGE'] == 0) & (test_results_df['DISTINCT_COUNT'] == 1)) |\n",
    "            (test_results_df['DISTINCT_COUNT'] == 0) |\n",
    "            (test_results_df['DISTINCT_VALUES'].isna()) #|\n",
    "            #(test_results_df['COLUMN_NAME'] == 'NON_APP_REFERRAL_DATE')\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    schema_mapped_df_encoding_copy = schema_mapped_df[~schema_mapped_df['COLUMN_NAME'].isin(IGNORE_COLUMN_NAMES)]\n",
    "    \n",
    "    Category_Mapping_Reference = {\n",
    "        'ALWAYS_CATEGORICAL': ['STRING', 'VARIANT', 'MapType', 'GEOGRAPHY'],\n",
    "        'ALWAYS_NUMERIC': ['BINARY'],\n",
    "        'POTENTIALLY_NUMERIC_ALL' : ['NUMBER', 'DATE', 'TIME'],\n",
    "        'IGNORE': ['OTHER']\n",
    "    }\n",
    "    \n",
    "    ALWAYS_CATEGORICAL_COLUMN_NAMES = schema_mapped_df_encoding_copy[schema_mapped_df_encoding_copy['TEST_DATA_TYPE'].isin(Category_Mapping_Reference['ALWAYS_CATEGORICAL'])]['COLUMN_NAME'].tolist()\n",
    "    ALWAYS_NUMERIC_COLUMN_NAMES = schema_mapped_df_encoding_copy[schema_mapped_df_encoding_copy['TEST_DATA_TYPE'].isin(Category_Mapping_Reference['ALWAYS_NUMERIC'])]['COLUMN_NAME'].tolist()\n",
    "    POTENTIALLY_NUMERIC_ALL_COLUMN_NAMES = schema_mapped_df_encoding_copy[schema_mapped_df_encoding_copy['TEST_DATA_TYPE'].isin(Category_Mapping_Reference['POTENTIALLY_NUMERIC_ALL'])]['COLUMN_NAME'].tolist()\n",
    "    IGNORE_COLUMN_NAMES += schema_mapped_df_encoding_copy[schema_mapped_df_encoding_copy['TEST_DATA_TYPE'].isin(Category_Mapping_Reference['IGNORE'])]['COLUMN_NAME'].tolist()\n",
    "    \n",
    "    #Decide if categorical or numeric based on # of distinct values\n",
    "    \n",
    "    # Filter test_results_df for rows where \"COLUMN_NAME\" is in POTENTIALLY_NUMERIC_ALL\n",
    "    categorical_test_results = filtered_test_results_df[filtered_test_results_df[\"COLUMN_NAME\"].isin(POTENTIALLY_NUMERIC_ALL_COLUMN_NAMES)].sort_values(by='DISTINCT_COUNT', ascending=False)\n",
    "    \n",
    "    # Further filter for rows where \"DISTINCT_COUNT\" < threshold\n",
    "    threshold = 10 \n",
    "    columns_to_add_to_numeric = categorical_test_results[categorical_test_results[\"DISTINCT_COUNT\"] > threshold][\"COLUMN_NAME\"].tolist()\n",
    "    columns_to_add_to_categorical = categorical_test_results[categorical_test_results[\"DISTINCT_COUNT\"] <= threshold][\"COLUMN_NAME\"].tolist()\n",
    "    \n",
    "    # Split up the 'potentially...' categories concretely into numeric or categorical\n",
    "    \n",
    "    NUMERIC_COLUMN_NAMES = ALWAYS_NUMERIC_COLUMN_NAMES + columns_to_add_to_numeric\n",
    "    CATEGORICAL_COLUMN_NAMES = ALWAYS_CATEGORICAL_COLUMN_NAMES + columns_to_add_to_categorical\n",
    "    \n",
    "    \n",
    "    CATEGORICAL_COLUMN_NAMES_plus_goal = CATEGORICAL_COLUMN_NAMES.copy()\n",
    "    \n",
    "    # Ensure 'CURRENT_VALUE' is the first element of CATEGORICAL_COLUMN_NAMES_plus_goal\n",
    "    if 'CURRENT_VALUE' not in CATEGORICAL_COLUMN_NAMES_plus_goal:\n",
    "        CATEGORICAL_COLUMN_NAMES_plus_goal.insert(0, 'CURRENT_VALUE')  # Adds 'CURRENT_VALUE' at the start\n",
    "    elif CATEGORICAL_COLUMN_NAMES_plus_goal.index('CURRENT_VALUE') != 0:\n",
    "        # If 'CURRENT_VALUE' is already in the list but not the first, move it to the start\n",
    "        CATEGORICAL_COLUMN_NAMES_plus_goal.remove('CURRENT_VALUE')\n",
    "        CATEGORICAL_COLUMN_NAMES_plus_goal.insert(0, 'CURRENT_VALUE')\n",
    "    \n",
    "    \n",
    "    print(\"Done!\")\n",
    "    print(\"\")\n",
    "    print(\"# of columns in original: \",len(schema_mapped_df)) \n",
    "    print(\"# of columns to test: \",len(NUMERIC_COLUMN_NAMES) + len(CATEGORICAL_COLUMN_NAMES))\n",
    "    print(\"# of numeric columns to test: \",len(NUMERIC_COLUMN_NAMES))\n",
    "    print(\"# of categorical columns to test: \",len(CATEGORICAL_COLUMN_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbf063c6-b590-45b5-84c2-4e5ce7e7fdbb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------    Convert Dates to Unix #s for Encoding (snowpark)     ----------------------------\n",
    "\n",
    "def convert_dates_to_unix_encoding ():    \n",
    "    print(\"#--------------------------    Convert Dates to Unix #s for Encoding (snowpark)     ----------------------------\")\n",
    "    #__Needed from previous functions__\n",
    "    #--load_data--\n",
    "    global enroll_stable_copy\n",
    "    #--map_data_types_to_categories--\n",
    "    global schema_mapped_df\n",
    "    #--classify_columns_for_corr_and_anova--\n",
    "    global NUMERIC_COLUMN_NAMES\n",
    "    global IGNORE_COLUMN_NAMES\n",
    "    \n",
    "    #__Set for the first time in this function__\n",
    "    global raw_data_snowpark_copy\n",
    "    global DATES_TO_CONVERT_TO_NUMERIC\n",
    "\n",
    "    \n",
    "    #___function logic starts here___\n",
    "    DATES_TO_CONVERT_TO_NUMERIC = schema_mapped_df[\n",
    "        (schema_mapped_df['COLUMN_NAME'].isin(NUMERIC_COLUMN_NAMES)) &\n",
    "        (schema_mapped_df['TEST_DATA_TYPE'] == 'DATE')\n",
    "    ]['COLUMN_NAME'].tolist()\n",
    "    \n",
    "    import pandas as pd\n",
    "    from snowflake.snowpark.session import Session\n",
    "    from snowflake.snowpark.functions import unix_timestamp, col\n",
    "    \n",
    "    \n",
    "    raw_data_snowpark_copy = enroll_stable_copy.drop(*IGNORE_COLUMN_NAMES)\n",
    "    \n",
    "    print(\"Convert date columns to Unix time in seconds (epoch time)\")\n",
    "    # Assuming all columns in 'filtered_df' are date columns to be converted\n",
    "    for column_name in DATES_TO_CONVERT_TO_NUMERIC:\n",
    "        raw_data_snowpark_copy = raw_data_snowpark_copy.withColumn(column_name, unix_timestamp(col(column_name)))\n",
    "    \n",
    "    # Display the updated Snowpark DataFrame\n",
    "    display(raw_data_snowpark_copy.to_pandas()[DATES_TO_CONVERT_TO_NUMERIC].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb9f926a-3b23-4c22-9d56-f6c3e889d301",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#--------------------------    Sort Category Values by Avg Goal/Value Metric     ----------------------------\n",
    "\n",
    "def sort_category_values_by_goal_metric():\n",
    "    print(\"#--------------------------    Sort Category Values by Avg Goal/Value Metric     ----------------------------\")\n",
    "    #__Needed from previous functions__\n",
    "    #--load_data--\n",
    "    #global enroll_stable_copy\n",
    "    #--map_data_types_to_categories--\n",
    "    #global schema_mapped_df\n",
    "    #--classify_columns_for_corr_and_anova--\n",
    "    global CATEGORICAL_COLUMN_NAMES\n",
    "    #--convert_dates_to_unix_encoding--\n",
    "    global raw_data_snowpark_copy\n",
    "    \n",
    "    #__Set for the first time in this function__\n",
    "    global sorted_category_averages\n",
    "\n",
    "    \n",
    "    #___function logic starts here___\n",
    "    def dynamic_sort_by_avg_current_value(df, categorical_columns):\n",
    "        index = 1\n",
    "        total_columns = len(categorical_columns)\n",
    "        \n",
    "        sorted_category_avg = {}\n",
    "        final_output_with_ranking = {}\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            \n",
    "            if index % 10 == 0:\n",
    "                print(f\"{index}/{total_columns}\")\n",
    "            index += 1\n",
    "            \n",
    "            # Group by each categorical column and calculate the average CURRENT_VALUE\n",
    "            df_grouped_avg = df.groupBy(col(column)).agg(avg(\"current_value\").alias(\"avg_current_value\"))\n",
    "    \n",
    "            # Sort the result by the average CURRENT_VALUE in descending order\n",
    "            df_sorted_avg = df_grouped_avg.sort(col(\"avg_current_value\").asc())\n",
    "    \n",
    "            # Collect the results\n",
    "            sorted_category_avg[column] = df_sorted_avg.collect()\n",
    "            sorted_values = df_sorted_avg.collect()\n",
    "    \n",
    "            #Save as {cat_column1: {'cat1':rank, 'cat2':rank...}, cat_column2...}\n",
    "            category_ranking = {value[column]: idx for idx, value in enumerate(sorted_values)}\n",
    "            \n",
    "            final_output_with_ranking[column] = category_ranking\n",
    "    \n",
    "        return final_output_with_ranking\n",
    "    \n",
    "    # Execute the function and print debugging information\n",
    "    sorted_category_averages = dynamic_sort_by_avg_current_value(raw_data_snowpark_copy, CATEGORICAL_COLUMN_NAMES)\n",
    "    \n",
    "    print(\"sample output:\")\n",
    "    print(sorted_category_averages[list(sorted_category_averages.keys())[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ed64951-4050-4fb2-a60b-f88f6f1439db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------    Convert Category Values to their Encoding     ----------------------------\n",
    "\n",
    "def convert_category_values_to_encoding():\n",
    "    #__Needed from previous functions__\n",
    "    #--load_data--\n",
    "    global enroll_stable_copy\n",
    "    #--map_data_types_to_categories--\n",
    "    #global schema_mapped_df\n",
    "    #--classify_columns_for_corr_and_anova--\n",
    "    global CATEGORICAL_COLUMN_NAMES\n",
    "    #--convert_dates_to_unix_encoding--\n",
    "    global raw_data_snowpark_copy\n",
    "    #--sort_category_values_by_goal_metric--\n",
    "    global sorted_category_averages\n",
    "    \n",
    "    #__Set for the first time in this function__\n",
    "    global encoded_df\n",
    "    global binary_encoded_df\n",
    "\n",
    "    #___function logic starts here___\n",
    "    # Convert the Snowflake DataFrame to a Pandas DataFrame for processing\n",
    "    try:\n",
    "        encoded_df = raw_data_snowpark_copy.toPandas()\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting Snowflake DataFrame to Pandas: {e}\")\n",
    "        # Exiting if conversion fails to avoid further errors\n",
    "        raise\n",
    "    \n",
    "    index = 1\n",
    "    # Apply mappings to replace categories in the DataFrame with their corresponding ranks\n",
    "    for column, cat_mapping in sorted_category_averages.items():\n",
    "        if index %10==0:\n",
    "            print(f\"{index}/{len(sorted_category_averages)}\")\n",
    "        index += 1\n",
    "        # Check if the column exists to avoid KeyError\n",
    "        if column in encoded_df.columns and column != 'CURRENT_VALUE':\n",
    "            # Map each category to its rank, retain original value for unmapped/non-categorical columns\n",
    "            encoded_df[column] = encoded_df[column].map(cat_mapping).fillna(encoded_df[column])\n",
    "        else:\n",
    "            # Inform if a specified column is missing in the DataFrame\n",
    "            print(f\"Column {column} not found in DataFrame.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    binary_encoded_df = enroll_stable_copy\n",
    "    all_columns = binary_encoded_df.columns\n",
    "    \n",
    "    # Apply transformation to all columns except the excluded one\n",
    "    # Replace nulls with 0 and non-nulls with 1 for the rest\n",
    "    transformed_columns = [\n",
    "        when(binary_encoded_df[col].is_not_null(), 1).otherwise(0).alias(col)\n",
    "        if col != 'CURRENT_VALUE'\n",
    "        else binary_encoded_df[col]\n",
    "        for col in all_columns\n",
    "    ]\n",
    "    \n",
    "    # Create a new pandas DataFrame with the transformed columns\n",
    "    binary_encoded_df = binary_encoded_df.select(transformed_columns).to_pandas()\n",
    "    \n",
    "    # Display the first few rows to verify\n",
    "    display(binary_encoded_df.head())\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # Convert the updated Pandas DataFrame back to a Snowflake DataFrame\n",
    "    try:\n",
    "        updated_enroll_stable_copy = session.createDataFrame(enroll_stable_copy_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting updated Pandas DataFrame back to Snowflake: {e}\")\n",
    "        # Exiting if conversion fails to prevent incomplete updates\n",
    "        raise\n",
    "    \n",
    "    # Attempt to write the updated DataFrame back to Snowflake, replacing the existing table\n",
    "    try:\n",
    "        updated_enroll_stable_copy.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "        # Confirm successful update\n",
    "        print(f\"Successfully updated table {table_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing updated DataFrame to Snowflake: {e}\")\n",
    "        # Exiting on failure to ensure data integrity\n",
    "        raise\n",
    "    ''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3b1f5-a0f2-41c9-9322-b4d04b84fa9f",
   "metadata": {},
   "source": [
    "#### - Run Corr Analysis (Numeric Conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb5c0faa-eca4-4546-b0d5-8e5c5b74795c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------    Perform Correlation Analysis (Standard Encoded)    ----------------------------\n",
    "\n",
    "def run_standard_encoded_corr_analysis():\n",
    "    #__Needed from previous functions__\n",
    "    #--load_data--\n",
    "    #global enroll_stable_copy\n",
    "    #--map_data_types_to_categories--\n",
    "    global schema_mapped_df\n",
    "    #--classify_columns_for_corr_and_anova--\n",
    "    global CATEGORICAL_COLUMN_NAMES\n",
    "    global NUMERIC_COLUMN_NAMES\n",
    "    global DATES_TO_CONVERT_TO_NUMERIC\n",
    "    #--convert_dates_to_unix_encoding--\n",
    "    global raw_data_snowpark_copy\n",
    "    #--sort_category_values_by_goal_metric--\n",
    "    global sorted_category_averages\n",
    "    #--convert_category_values_to_encoding--\n",
    "    global encoded_df\n",
    "    \n",
    "    #__Set for the first time in this function__\n",
    "    global standard_pearson_corr_results\n",
    "\n",
    "    #___function logic starts here___\n",
    "    print('starting...')\n",
    "    \n",
    "    #categorical_test_data = encoded_df[CATEGORICAL_COLUMN_NAMES].head(10)\n",
    "    #date_test_data = encoded_df[DATES_TO_CONVERT_TO_NUMERIC].head(10)\n",
    "    #NON_DATE_NUMERIC_COLUMNS = list(set(NUMERIC_COLUMN_NAMES) - set(DATES_TO_CONVERT_TO_NUMERIC))\n",
    "    #numeric_test_data = encoded_df[NON_DATE_NUMERIC_COLUMNS].replace({True: 1, False: 0})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #raw_all_pearson_corr = encoded_df.corr(method='pearson', numeric_only = False)\n",
    "    #raw_all_spearman_corr = encoded_df.corr(method='spearman', numeric_only = False)\n",
    "    #raw_all_kendall_corr = encoded_df.corr(method='kendall', numeric_only = False)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    print(datetime.now())\n",
    "    \n",
    "    #from snowflake.ml.modeling.metrics import correlation\n",
    "    \n",
    "    print(\"# of columns in original: \",len(schema_mapped_df)) \n",
    "    print(\"# of columns to test: \",len(NUMERIC_COLUMN_NAMES) + len(CATEGORICAL_COLUMN_NAMES))\n",
    "    print(\"# of numeric columns to test: \",len(NUMERIC_COLUMN_NAMES))\n",
    "    print(\"# of numeric (date) columns to test: \", len(DATES_TO_CONVERT_TO_NUMERIC))\n",
    "    print(\"# of categorical columns to test: \",len(CATEGORICAL_COLUMN_NAMES))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"running corr on pandas df...\")\n",
    "    standard_pearson_corr_results = encoded_df.corr(method='pearson', numeric_only = False)\n",
    "    print(datetime.now())\n",
    "    duration = datetime.now() - start_time\n",
    "    print(duration.total_seconds())\n",
    "    \n",
    "    print(\"update structure\")\n",
    "    standard_pearson_corr_results = standard_pearson_corr_results.dropna(axis=1, how='all')\n",
    "    standard_pearson_corr_results = standard_pearson_corr_results.dropna(axis=0, how='all')\n",
    "    standard_pearson_corr_results = standard_pearson_corr_results.reset_index()\n",
    "    standard_pearson_corr_results.rename(columns={standard_pearson_corr_results.columns[0]: 'COLUMN_NAME'}, inplace=True)\n",
    "    print(datetime.now())\n",
    "    duration = datetime.now() - start_time\n",
    "    print(duration.total_seconds())\n",
    "    \n",
    "    print(\"displaying the results...\")\n",
    "    display(standard_pearson_corr_results)\n",
    "    print(datetime.now())\n",
    "    duration = datetime.now() - start_time\n",
    "    print(duration.total_seconds())\n",
    "    \n",
    "    print(\"saving to csv...\")\n",
    "    standard_pearson_corr_results.to_csv(r'C:\\Users\\miwilliams\\Downloads\\pandas_corr.csv', index=False)\n",
    "    print(datetime.now())\n",
    "    duration = datetime.now() - start_time\n",
    "    print(duration.total_seconds())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    set_a = set(categorical_test_data.columns)\n",
    "    set_b = set(date_test_data.columns)\n",
    "    set_c = set(non_categorical_or_date_test_data.columns)\n",
    "    \n",
    "    # Find the unique elements in each set and their intersections\n",
    "    unique_to_a = set_a - (set_b | set_c)\n",
    "    unique_to_b = set_b - (set_a | set_c)\n",
    "    unique_to_c = set_c - (set_a | set_b)\n",
    "    common_to_all = set_a & set_b & set_c\n",
    "    common_to_a_b = (set_a & set_b) - set_c\n",
    "    common_to_a_c = (set_a & set_c) - set_b\n",
    "    common_to_b_c = (set_b & set_c) - set_a\n",
    "    \n",
    "    # Combine results in a dictionary for clarity\n",
    "    comparison_results = {\n",
    "        #\"unique_to_a\": unique_to_a,\n",
    "        \"unique_to_b\": unique_to_b,\n",
    "        \"unique_to_c\": unique_to_c,\n",
    "        #\"common_to_all\": common_to_all,\n",
    "        #\"common_to_a_b\": common_to_a_b,\n",
    "        #\"common_to_a_c\": common_to_a_c,\n",
    "        \"common_to_b_c\": common_to_b_c\n",
    "    }\n",
    "    \n",
    "    pprint(comparison_results)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"pushing to snowflake...\")\n",
    "    #encoded_df_snowflake = session.write_pandas(encoded_df, \"bi_enroll_data_correlation_test\", auto_create_table=True)\n",
    "    #print(datetime.now())\n",
    "    #duration = datetime.now() - start_time\n",
    "    #print(duration.total_seconds())\n",
    "          \n",
    "    #print(\"starting correlation analysis...\")\n",
    "    #correlation_matrix_df = correlation(df=encoded_df_snowflake)\n",
    "    #print(datetime.now())\n",
    "    #duration = datetime.now() - start_time\n",
    "    #print(duration.total_seconds())\n",
    "    \n",
    "    #print(\"update structure\")\n",
    "    #correlation_matrix_df = correlation_matrix_df.dropna(axis=1, how='all')\n",
    "    #correlation_matrix_df = correlation_matrix_df.dropna(axis=0, how='all')\n",
    "    #correlation_matrix_df = correlation_matrix_df.reset_index()\n",
    "    #correlation_matrix_df.rename(columns={correlation_matrix_df.columns[0]: 'COLUMN_NAME'}, inplace=True)\n",
    "    #print(datetime.now())\n",
    "    #duration = datetime.now() - start_time\n",
    "    #print(duration.total_seconds())\n",
    "    \n",
    "    #print(\"displaying the results...\")\n",
    "    #display(correlation_matrix_df)\n",
    "    #print(datetime.now())\n",
    "    #duration = datetime.now() - start_time\n",
    "    #print(duration.total_seconds())\n",
    "    \n",
    "    #print(\"saving to csv...\")\n",
    "    #correlation_matrix_df.to_csv(r'C:\\Users\\miwilliams\\Downloads\\snowflake_corr.csv', index=False)\n",
    "    #print(datetime.now())\n",
    "    #duration = datetime.now() - start_time\n",
    "    #print(duration.total_seconds())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n------------------------------------------------------------------------------\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "    \n",
    "    print(\"categorical corr tests:\")\n",
    "    categorical_pearson_corr = categorical_test_data.corr(method='pearson', numeric_only = False)\n",
    "    categorical_spearman_corr = categorical_test_data.corr(method='spearman', numeric_only = False)\n",
    "    categorical_kendall_corr = categorical_test_data.corr(method='kendall', numeric_only = False)\n",
    "    \n",
    "    display(categorical_pearson_corr)\n",
    "    \n",
    "    print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n------------------------------------------------------------------------------\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "    print(\"date corr tests:\")\n",
    "    date_pearson_corr = date_test_data.corr(method='pearson', numeric_only = False)\n",
    "    date_spearman_corr = date_test_data.corr(method='spearman', numeric_only = False)\n",
    "    date_kendall_corr = date_test_data.corr(method='kendall', numeric_only = False)\n",
    "    \n",
    "    display(date_pearson_corr)\n",
    "    \n",
    "    print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n------------------------------------------------------------------------------\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "    \n",
    "    print(\"numeric corr tests:\")\n",
    "    numeric_pearson_corr = numeric_test_data.corr(method='pearson', numeric_only = False)\n",
    "    numeric_spearman_corr = numeric_test_data.corr(method='spearman', numeric_only = False)\n",
    "    numeric_kendall_corr = numeric_test_data.corr(method='kendall', numeric_only = False)\n",
    "    \n",
    "    display(numeric_pearson_corr)\n",
    "    '''\n",
    "    \n",
    "    print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba51cb-6a55-4edd-8876-c5a85baac1c6",
   "metadata": {},
   "source": [
    "#### - Run Corr Analysis (Binary Conversion - Null/Populated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db8f634d-650d-4d31-9a01-913505b92d59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------    Perform Correlation Analysis (Binary)    ----------------------------\n",
    "\n",
    "def run_binary_encoded_corr_analysis():\n",
    "    #__Needed from previous functions__\n",
    "    #--convert_category_values_to_encoding--\n",
    "    global binary_encoded_df\n",
    "    \n",
    "    #__Set for the first time in this function__\n",
    "    global binary_pearson_corr_results\n",
    "\n",
    "    #___function logic starts here___\n",
    "    '''Inputs:'''\n",
    "    #enroll_stable_copy\n",
    "    '''Outputs:'''\n",
    "    #binary_df\n",
    "    #raw_all_pearson_corr_binary\n",
    "    \n",
    "    print('displaying binary_encoded_df...')\n",
    "    display(binary_encoded_df.head())\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    print(start_time)\n",
    "    \n",
    "    print(\"running corr on pandas df...\")\n",
    "    binary_pearson_corr_results = binary_encoded_df.corr(method='pearson', numeric_only = False)\n",
    "    print(datetime.now())\n",
    "    duration = datetime.now() - start_time\n",
    "    print(duration.total_seconds())\n",
    "    \n",
    "    print(\"update structure\")\n",
    "    binary_pearson_corr_results = binary_pearson_corr_results.dropna(axis=1, how='all')\n",
    "    binary_pearson_corr_results = binary_pearson_corr_results.dropna(axis=0, how='all')\n",
    "    binary_pearson_corr_results = binary_pearson_corr_results.reset_index()\n",
    "    binary_pearson_corr_results.rename(columns={binary_pearson_corr_results.columns[0]: 'COLUMN_NAME'}, inplace=True)\n",
    "    print(datetime.now())\n",
    "    duration = datetime.now() - start_time\n",
    "    print(duration.total_seconds())\n",
    "    \n",
    "    print(\"displaying the results...\")\n",
    "    display(binary_pearson_corr_results)\n",
    "    print(datetime.now())\n",
    "    duration = datetime.now() - start_time\n",
    "    print(duration.total_seconds())\n",
    "    \n",
    "    print(\"saving to csv...\")\n",
    "    binary_pearson_corr_results.to_csv(r'C:\\Users\\miwilliams\\Downloads\\pandas_corr_binary.csv', index=False)\n",
    "    print(datetime.now())\n",
    "    duration = datetime.now() - start_time\n",
    "    print(duration.total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49acff-ec3e-4b58-920d-08b7f1d1d21c",
   "metadata": {},
   "source": [
    "#### - Impute Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9102574-7d28-42c0-94e9-8ed13a1b0418",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------    Impute Nulls    ----------------------------\n",
    "\n",
    "imputed_df = None\n",
    "def impute_nulls():\n",
    "    #__Needed from previous functions__\n",
    "    #--convert_category_values_to_encoding--\n",
    "    global encoded_df\n",
    "    global binary_encoded_df\n",
    "    \n",
    "    #__Set for the first time in this function__\n",
    "    global imputed_df\n",
    "\n",
    "    #___function logic starts here___\n",
    "    print(\"impute_nulls running...\")\n",
    "    #from sklearn.impute import KNNImputer <- Dope, but takes too long\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    # Removing all raw unix date columns from the encoded_df\n",
    "    encoded_df_no_dates = encoded_df.drop(columns=DATES_TO_CONVERT_TO_NUMERIC)\n",
    "    \n",
    "    # Renaming binary_encoded_df columns to add '_binary' suffix\n",
    "    binary_encoded_df_renamed = encoded_df_no_dates.add_suffix('_binary')\n",
    "    \n",
    "    # Merging the modified encoded_df with the renamed binary_encoded_df\n",
    "    combined_encoded_df = pd.concat([encoded_df_no_dates, binary_encoded_df_renamed], axis=1)\n",
    "    \n",
    "    \n",
    "    print('1')\n",
    "    imputed_data = SimpleImputer(strategy='mean').fit_transform(combined_encoded_df) #median, most_frequent\n",
    "    print('2')\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=combined_encoded_df.columns)\n",
    "    \n",
    "    # Display the imputed DataFrame\n",
    "    print(\"Imputed DataFrame:\")\n",
    "    display(imputed_df.head())\n",
    "#impute_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff492cdd-c6ba-4ea7-a017-5dcfb70b77a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#--------------------------    Calculate MLR    ----------------------------\n",
    "\n",
    "results_df = {}\n",
    "best_model_details = None\n",
    "X_imputed_df = None\n",
    "def calculate_MLR():\n",
    "    #__Needed from previous functions__\n",
    "    #--impute_nulls--\n",
    "    global imputed_df\n",
    "    \n",
    "    #__Set for the first time in this function__\n",
    "    global results_dict\n",
    "    global best_model_details\n",
    "    global X_imputed_df\n",
    "\n",
    "    #___function logic starts here___\n",
    "    import statsmodels.api as sm\n",
    "    from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    import warnings\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    import re\n",
    "\n",
    "    # Split the data into features and target for MLR\n",
    "    print('Preparing data for modeling...')\n",
    "    X = imputed_df.drop('CURRENT_VALUE', axis=1)\n",
    "    y = imputed_df['CURRENT_VALUE']\n",
    "    X_imputed_df = X \n",
    "    \n",
    "    results_df = {}\n",
    "    \n",
    "    # Using sklearn for Lasso Regression\n",
    "    def attempt_MLR(optimal_alpha, optimal_max_iter, optimal_tol, conservative_alpha, conservative_max_iter, conservative_tol):\n",
    "        results = {\n",
    "            'optimal': {'converged': False, 'model': None, 'r_squared': None, 'train_time': None, 'duality_gap': None, 'num_sparse_coefs': None},\n",
    "            'conservative': {'converged': False, 'model': None, 'r_squared': None, 'train_time': None, 'duality_gap': None, 'num_sparse_coefs': None}\n",
    "        }\n",
    "    \n",
    "        # Function to extract duality gap from warning message\n",
    "        def extract_duality_gap(warn_message):\n",
    "            match = re.search(r\"Duality gap: ([\\d.e+]+)\", warn_message)\n",
    "            if match:\n",
    "                print('Did not converge. Successfully extracted duality gap.')\n",
    "                return float(match.group(1))\n",
    "            print('Did not converge. Could not extract duality gap.')\n",
    "            return None\n",
    "    \n",
    "        # Optimistic attempt\n",
    "        start_time = time()\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "            try:\n",
    "                model_sk_lasso_optimistic = Lasso(alpha=optimal_alpha, max_iter=optimal_max_iter, tol=optimal_tol).fit(X, y)\n",
    "                r_squared_lasso_optimistic = model_sk_lasso_optimistic.score(X, y)\n",
    "                num_sparse_coefs_optimistic = np.sum(model_sk_lasso_optimistic.coef_ != 0)\n",
    "                results['optimal'].update({'converged': True, 'model': model_sk_lasso_optimistic, 'r_squared': r_squared_lasso_optimistic, 'train_time': time() - start_time, 'duality_gap': model_sk_lasso_optimistic.dual_gap_, 'num_sparse_coefs': num_sparse_coefs_optimistic})\n",
    "                print(f\"Optimal converged successfully\")\n",
    "            except ConvergenceWarning as e:\n",
    "                print(f\"Optimistic Lasso failed to converge: {e}\")\n",
    "                results['optimal'].update({'converged': False, 'model': model_sk_lasso_optimistic, 'r_squared': r_squared_lasso_optimistic, 'train_time': time() - start_time, 'duality_gap': model_sk_lasso_optimistic.dual_gap_, 'num_sparse_coefs': num_sparse_coefs_optimistic})\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {e}\")\n",
    "                results['optimal'].update({'train_time': time() - start_time})\n",
    "        \n",
    "        # Conservative attempt\n",
    "        start_time = time()\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "            try:\n",
    "                model_sk_lasso_conservative = Lasso(alpha=conservative_alpha, max_iter=conservative_max_iter, tol=conservative_tol).fit(X, y)\n",
    "                r_squared_lasso_conservative = model_sk_lasso_conservative.score(X, y)\n",
    "                num_sparse_coefs_conservative = np.sum(model_sk_lasso_conservative.coef_ != 0)\n",
    "                results['conservative'].update({'converged': True, 'model': model_sk_lasso_conservative, 'r_squared': r_squared_lasso_conservative, 'train_time': time() - start_time, 'duality_gap': model_sk_lasso_conservative.dual_gap_, 'num_sparse_coefs': num_sparse_coefs_conservative})\n",
    "                print(f\"Conservative converged successfully\")\n",
    "            except ConvergenceWarning as e:\n",
    "                print(f\"Conservative Lasso failed to converge: {e}\")\n",
    "                results['conservative'].update({'converged': False, 'model': model_sk_lasso_conservative, 'r_squared': r_squared_lasso_conservative, 'train_time': time() - start_time, 'duality_gap': model_sk_lasso_conservative.dual_gap_, 'num_sparse_coefs': num_sparse_coefs_conservative})\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {e}\")\n",
    "                results['conservative'].update({'train_time': time() - start_time})\n",
    "    \n",
    "        return results\n",
    "\n",
    "\n",
    "    # Additional Function to Refine Parameters Based on Previous Results\n",
    "    def iterative_refinement():\n",
    "        global results_dict\n",
    "        global best_model_details\n",
    "        # Initialize DataFrame to store results\n",
    "        columns = ['iteration', 'type', 'alpha', 'max_iter', 'tol', 'train_time', 'num_sparse_coefs', 'duality_gap', 'r_squared', 'converged_goal_met', 'r_squared_goal_gap']\n",
    "\n",
    "        results_dict = {}\n",
    "        best_model_details = None\n",
    "    \n",
    "        # Initial Parameters\n",
    "        optimal_alpha = 0.1\n",
    "        optimal_max_iter = 500\n",
    "        optimal_tol = 0.0001\n",
    "        conservative_alpha = 1\n",
    "        conservative_max_iter = 1000\n",
    "        conservative_tol = 0.0001\n",
    "        r_squared_goal = 0.75  # Define the goal for r_squared\n",
    "        \n",
    "        for iteration in range(1, 3):  # Iteration count starts at 1\n",
    "            print(f\"\\nIteration {iteration}:\")\n",
    "    \n",
    "            # Run Lasso regression with current parameters\n",
    "            attempt_results = attempt_MLR(optimal_alpha, optimal_max_iter, optimal_tol,\n",
    "                                          conservative_alpha, conservative_max_iter, conservative_tol)\n",
    "\n",
    "            results_dict[iteration] = attempt_results\n",
    "            \n",
    "\n",
    "            for model_type, model_details in attempt_results.items():\n",
    "                if model_details['converged']:\n",
    "                    if best_model_details is None:\n",
    "                        best_model_details = model_details\n",
    "                    else:\n",
    "                        # Compare with current best model based on r_squared and num_sparse_coefs\n",
    "                        is_better_r_squared = model_details['r_squared'] > best_model_details['r_squared']\n",
    "                        is_equal_r_squared = model_details['r_squared'] == best_model_details['r_squared']\n",
    "                        is_less_sparse_coefs = model_details['num_sparse_coefs'] < best_model_details['num_sparse_coefs']\n",
    "    \n",
    "                        if is_better_r_squared or (is_equal_r_squared and is_less_sparse_coefs):\n",
    "                            best_model_details = model_details\n",
    "\n",
    "            \n",
    "            '''\n",
    "            # Adjust parameters based on the outcome\n",
    "            for model_key in ['optimal', 'conservative']:\n",
    "                model_results = attempt_results[model_key]\n",
    "                if not model_results['converged']:\n",
    "                    if model_key == 'optimal':\n",
    "                        optimal_alpha *= 0.75  # Decrease alpha for optimal model\n",
    "                        optimal_max_iter += 500  # Increase max_iter for more chances to converge\n",
    "                    else:\n",
    "                        conservative_max_iter += 500  # Increase max_iter for conservative model\n",
    "                else:\n",
    "                    if model_results['r_squared'] < r_squared_goal:\n",
    "                        # Increase max_iter to improve fit\n",
    "                        if model_key == 'optimal':\n",
    "                            optimal_max_iter += 500\n",
    "                        else:\n",
    "                            conservative_max_iter += 500\n",
    "                    if model_results.get('duality_gap') is not None and model_results['duality_gap'] > 1000:\n",
    "                        # Duality gap too high, adjust tolerance and max_iter\n",
    "                        if model_key == 'optimal':\n",
    "                            optimal_tol *= 0.75\n",
    "                            optimal_max_iter += 500\n",
    "                        else:\n",
    "                            conservative_tol *= 0.75\n",
    "                            conservative_max_iter += 500\n",
    "    \n",
    "            # Evaluate and adjust based on sparsity\n",
    "            optimal_model = attempt_results['optimal'].get('model')\n",
    "            conservative_model = attempt_results['conservative'].get('model')\n",
    "            if optimal_model and np.sum(optimal_model.coef_ != 0) < 10:  # Example condition for sparsity\n",
    "                optimal_alpha *= 1.1  # Increase alpha to promote sparsity\n",
    "            if conservative_model and np.sum(conservative_model.coef_ != 0) > 50:\n",
    "                conservative_alpha *= 0.9  # Decrease alpha to allow more features\n",
    "    \n",
    "            # Condition to exit the loop if certain criteria are met\n",
    "            if attempt_results['optimal']['r_squared'] > 0.75 and attempt_results['conservative']['r_squared'] > 0.75:\n",
    "                print(\"Both models have satisfactory R-squared values. Ending iterations.\")\n",
    "                break\n",
    "            '''\n",
    "    \n",
    "    iterative_refinement()\n",
    "#calculate_MLR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afc5814b-df58-433e-b528-2b2544743059",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------    Extract Model    ----------------------------\n",
    "\n",
    "\n",
    "def extract_model():\n",
    "    global sorted_category_averages\n",
    "    global X_imputed_df\n",
    "    global best_model_details\n",
    "\n",
    "    \n",
    "    print(\"pd.DataFrame.from_dict(results_dict)\")\n",
    "    \n",
    "    print(\"best_model_details\")\n",
    "    print(best_model_details)\n",
    "    \n",
    "    best_model = best_model_details['model']\n",
    "    \n",
    "    print(\"Intercept:\")\n",
    "    print(best_model.intercept_)\n",
    "    \n",
    "    coefficients = best_model.coef_\n",
    "    \n",
    "    feature_names = X_imputed_df.columns\n",
    "\n",
    "    # Mapping coefficients to feature names\n",
    "    feature_coefficients = dict(zip(feature_names, coefficients))\n",
    "    feature_coefficients = dict(sorted(feature_coefficients.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    \n",
    "    #print(\"Feature Coefficients:\")\n",
    "    #for feature, coef in feature_coefficients.items():\n",
    "    #    print(f\"{feature}: {coef}\")\n",
    "    \n",
    "    \n",
    "    sparse_feature_coefficients = {feature: coef for feature, coef in feature_coefficients.items() if coef != 0}\n",
    "    removed_feature_coefficients = {feature: coef for feature, coef in feature_coefficients.items() if coef == 0}\n",
    "    \n",
    "    \n",
    "    sparse_category_cipher = {key: sorted_category_averages[key] for key in sorted_category_averages if key in sparse_feature_coefficients}\n",
    "    #print(\"sparse_category_cipher.keys()\")\n",
    "    #print(sparse_category_cipher.keys())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"Sparse Feature Coefficients:\")\n",
    "    #for feature, coef in sparse_feature_coefficients.items():\n",
    "    #    print(f\"{feature}: {coef}\")\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Assuming 'coefficients' and 'feature_names' are defined as above\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(feature_names)), coefficients)\n",
    "    plt.yticks(range(len(feature_names)), feature_names)\n",
    "    plt.xlabel(\"Coefficient Value\")\n",
    "    plt.ylabel(\"Feature Name\")\n",
    "    plt.title(\"Lasso Coefficients\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Predict using the best model\n",
    "    estimated_goals = best_model.predict(X_imputed_df)\n",
    "\n",
    "    # Add the prediction results as a new column to imputed_df\n",
    "    imputed_df['estimated goal'] = estimated_goals\n",
    "    \n",
    "    \n",
    "    # Optional: Print or display a portion of imputed_df to verify the new column\n",
    "    showall(imputed_df[['estimated goal', 'CURRENT_VALUE']])\n",
    "#extract_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0710e52c-e13a-4d37-beff-761d0d4f9aac",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Placeholder for Step 1.3: Adjusting based on feedback\n",
    "    # Here you would adjust the alpha values based on the results of the initial fits,\n",
    "    # potentially in a loop or iteratively adjusting until a satisfactory starting point is found.\n",
    "    \n",
    "    # Note: For actual implementation, consider dynamically adjusting the alpha (and other parameters) \n",
    "    # based on performance metrics and convergence status.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Using sklearn for Elastic Net Regression\n",
    "    #print('10')\n",
    "    #model_sk_enet = ElasticNet(alpha=0.1, l1_ratio=0.5, verbose=True).fit(X, y)  # l1_ratio controls the mix of L1 and L2 regularization\n",
    "    #print('11')\n",
    "    #r_squared_enet = model_sk_enet.score(X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #--------- simple, maybe later/for comparison ---------\n",
    "\n",
    "    # Using statsmodels for MLR\n",
    "    #print('4')\n",
    "    #X_sm = sm.add_constant(X)  # adding a constant\n",
    "    #print('5')\n",
    "    #model_sm = sm.OLS(y, X_sm).fit()\n",
    "    \n",
    "    # Using sklearn for MLR\n",
    "    #print('6')\n",
    "    #model_sk = LinearRegression().fit(X, y)\n",
    "    #print('7')\n",
    "    #r_squared_sk = model_sk.score(X, y)\n",
    "    \n",
    "    # Print the statsmodels summary\n",
    "    #print(\"\\nStatsmodels Summary:\")\n",
    "    #print(model_sm.summary())\n",
    "    \n",
    "    # Print the sklearn R-squared value\n",
    "    #print(\"\\nSklearn R-squared:\", r_squared_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d21638-e052-4a06-ab11-7008e0b25891",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Master Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6613fc8-1266-4b62-8ac1-b7c6e76680a0",
   "metadata": {},
   "source": [
    "#### - Master Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81abd6ab-3b6d-4648-ba97-a051e723a73f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Controller Starting..\n",
      "--Getting user inputs/creating session--\n",
      "no existing session, creating new...\n",
      "\n",
      "snowpark API response...\n",
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Going to open: https://aledade.okta.com/app/snowflake/exkd0psxuu9C0LKlN697/sso/saml?SAMLRequest=lZJfb9owFMW%2FSuQ9J3ZCgWIBVQariko7VlI29c2NDfXi2MHXIfDt5%2FBn6h5aaW%2BRc45%2Fx%2Ffc4c2%2BVMFOWJBGj1AcERQInRsu9WaEnrPb8BoF4JjmTBktRuggAN2Mh8BKVdG0dm%2F6SWxrAS7wF2mg7Y8Rqq2mhoEEqlkpgLqcLtOHOU0iQhmAsM7j0NnCQXrWm3MVxbhpmqjpRMZucEIIwWSAvaqVfEHvENXnjMoaZ3KjLpa9f9MHiBiTqxbhFZ6wOBu%2FSn0awWeU15MI6F2WLcLF92WGgvTyuonRUJfCLoXdyVw8P81PAcAn%2BDFPr0ky6EU1hIKBC%2BMItGnWihUiN2VVO39t5L%2FwWnCszEb6Yc2mI1QVkhsmXkzn931B%2BmxVFl223Xbvfh4yu2oeSLr69Y3vVrxXCPna5ChYXapN2mpnALWY6bZQ549IchWSbhh3s%2FiadghNBlE%2FJi8omPpCpWbu6LykZkpwxkVkCseO4VhV4b%2B5sdgXnFSwr%2BvBhMzv1WNv0McABrd9odPK0GMAO%2F7vQQzxe%2Ft5%2FR59I7PpwiiZH4JbY0vmPi4sjuLjieTh%2BiilomRSpZxbAeCLU8o0EyuY81vubC0QHp%2Bo%2F%2B75%2BA8%3D&RelayState=61128 to authenticate...\n",
      "\n",
      "session created\n",
      "\n",
      "Database: EDW_PROD\n",
      "Schema: ANALYTICS\n",
      "--complete--\n",
      "\n",
      "Converting Date Fields...\n",
      "converting potential date fields...\n",
      "string columns pulled\n",
      "string column data queried\n",
      "Column 'COMBINED_LIVING_WILL_MPOA_DATE' successfully converted to datetime.\n",
      "Column 'SECOND_WAVE_ENGAGEMENT_DATE' successfully converted to datetime.\n",
      "Column 'DOCUMENT_COMPLETION_STATUS' successfully converted to datetime.\n",
      "Column 'PLANNING_SUMMARY_DATE' successfully converted to datetime.\n",
      "Column 'LIVING_WILL_DATE' successfully converted to datetime.\n",
      "Column 'DNR_DATE' successfully converted to datetime.\n",
      "--complete--\n",
      "\n",
      "--Adding Normalized Dates--\n",
      "executing sql\n",
      "calling stored procedure\n",
      "result is:\n",
      "[Row(DYNAMIC_DATE_DIFF_AND_UNION_SAVE_AS_TABLE='Table \"ANALYTICS\".\"BI_ENROLL_RATE_FULL_2\" created successfully with columns in the desired order.')]\n",
      "--complete--\n",
      "\n",
      "--Adding date sequences--\n",
      "add_date_sequences running...\n",
      "date_column_names\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_COMPLETION_STATUS</th>\n",
       "      <th>OPERATIONAL_ACO</th>\n",
       "      <th>MARKET</th>\n",
       "      <th>CURRENT_PRACTICE</th>\n",
       "      <th>PRAC_PRIMARY_FACILITY_TYPE</th>\n",
       "      <th>SECONDARY_REFERRAL_SOURCE</th>\n",
       "      <th>CONTRACT_PRAC_PRIMARY_FACILITY_TYPE</th>\n",
       "      <th>REFERRAL_SOURCE</th>\n",
       "      <th>TARGETED_CONTRACT_MSSP_TRACK_GROUPED</th>\n",
       "      <th>ACO_DISPLAY_NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>TARGETED_PT_CONTRACT_TYPE</th>\n",
       "      <th>CURRENT_ENGAGEMENT_STATUS</th>\n",
       "      <th>CONTRACTED_ENTITY</th>\n",
       "      <th>IRIS_UNABLE_TO_SCHEDULE_REASON</th>\n",
       "      <th>SECOND_WAVE_ENGAGEMENT_DATE</th>\n",
       "      <th>PRAC_OR_LOCATION</th>\n",
       "      <th>REFER_TO_PARTNER_BY</th>\n",
       "      <th>ORDERED_DATE_COL_NAMES</th>\n",
       "      <th>ORDERED_DATES</th>\n",
       "      <th>ORDERED_NORMALIZED_NUMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>KS MSSP Enhanced ACO</td>\n",
       "      <td>Kansas-Oklahoma</td>\n",
       "      <td>238</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PARTNER_ALGORITHM</td>\n",
       "      <td>MA</td>\n",
       "      <td>KS MSSP Enhanced (A2916)</td>\n",
       "      <td>...</td>\n",
       "      <td>MA</td>\n",
       "      <td>TAGGED_FOR_CACP</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>prac_name</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>MD Chesapeake ACO</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>1548</td>\n",
       "      <td>FQHC</td>\n",
       "      <td>None</td>\n",
       "      <td>FQHC</td>\n",
       "      <td>DIRECT_REFERRAL</td>\n",
       "      <td>MSSP ENHANCED</td>\n",
       "      <td>Chesapeake MSSP (A5003)</td>\n",
       "      <td>...</td>\n",
       "      <td>MSSP</td>\n",
       "      <td>PATIENT_DECLINED_SERVICE_TO_IRIS</td>\n",
       "      <td>205860113</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>West Cecil Health Center Inc (6261)</td>\n",
       "      <td>prac_name</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>WA ACO</td>\n",
       "      <td>Washington-Oregon</td>\n",
       "      <td>1756</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PARTNER_ALGORITHM</td>\n",
       "      <td>NO ATTRIBUTION</td>\n",
       "      <td>CA MSSP 2021 Enhanced (A4777)</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>TAGGED_FOR_CACP</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>prac_name</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>OH-PA ACO</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>902</td>\n",
       "      <td>Private Practice</td>\n",
       "      <td>None</td>\n",
       "      <td>Private Practice</td>\n",
       "      <td>PARTNER_ALGORITHM</td>\n",
       "      <td>MSSP ENHANCED</td>\n",
       "      <td>PA MSSP Legacy + Gateway Enhanced (A3457)</td>\n",
       "      <td>...</td>\n",
       "      <td>MSSP</td>\n",
       "      <td>PATIENT_DECLINED_SERVICE_TO_IRIS</td>\n",
       "      <td>141965232</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Adult Geriatrics of Wooster (902)</td>\n",
       "      <td>prac_name</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>NC East ACO</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>942</td>\n",
       "      <td>Private Practice</td>\n",
       "      <td>None</td>\n",
       "      <td>Private Practice</td>\n",
       "      <td>PARTNER_ALGORITHM</td>\n",
       "      <td>MA</td>\n",
       "      <td>Non-MSSP</td>\n",
       "      <td>...</td>\n",
       "      <td>MA</td>\n",
       "      <td>IRIS_OUTREACH_IN_PROGRESS</td>\n",
       "      <td>834172038</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-04-11</td>\n",
       "      <td>Wolinsky Primary Care (942)</td>\n",
       "      <td>prac_name</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  DOCUMENT_COMPLETION_STATUS       OPERATIONAL_ACO             MARKET  \\\n",
       "0                       None  KS MSSP Enhanced ACO    Kansas-Oklahoma   \n",
       "1                       None     MD Chesapeake ACO           Maryland   \n",
       "2                       None                WA ACO  Washington-Oregon   \n",
       "3                       None             OH-PA ACO               Ohio   \n",
       "4                       None           NC East ACO     North Carolina   \n",
       "\n",
       "  CURRENT_PRACTICE PRAC_PRIMARY_FACILITY_TYPE SECONDARY_REFERRAL_SOURCE  \\\n",
       "0              238                       None                      None   \n",
       "1             1548                       FQHC                      None   \n",
       "2             1756                       None                      None   \n",
       "3              902           Private Practice                      None   \n",
       "4              942           Private Practice                      None   \n",
       "\n",
       "  CONTRACT_PRAC_PRIMARY_FACILITY_TYPE    REFERRAL_SOURCE  \\\n",
       "0                                None  PARTNER_ALGORITHM   \n",
       "1                                FQHC    DIRECT_REFERRAL   \n",
       "2                                None  PARTNER_ALGORITHM   \n",
       "3                    Private Practice  PARTNER_ALGORITHM   \n",
       "4                    Private Practice  PARTNER_ALGORITHM   \n",
       "\n",
       "  TARGETED_CONTRACT_MSSP_TRACK_GROUPED  \\\n",
       "0                                   MA   \n",
       "1                        MSSP ENHANCED   \n",
       "2                       NO ATTRIBUTION   \n",
       "3                        MSSP ENHANCED   \n",
       "4                                   MA   \n",
       "\n",
       "                            ACO_DISPLAY_NAME  ... TARGETED_PT_CONTRACT_TYPE  \\\n",
       "0                   KS MSSP Enhanced (A2916)  ...                        MA   \n",
       "1                    Chesapeake MSSP (A5003)  ...                      MSSP   \n",
       "2              CA MSSP 2021 Enhanced (A4777)  ...                      None   \n",
       "3  PA MSSP Legacy + Gateway Enhanced (A3457)  ...                      MSSP   \n",
       "4                                   Non-MSSP  ...                        MA   \n",
       "\n",
       "          CURRENT_ENGAGEMENT_STATUS CONTRACTED_ENTITY  \\\n",
       "0                   TAGGED_FOR_CACP              None   \n",
       "1  PATIENT_DECLINED_SERVICE_TO_IRIS         205860113   \n",
       "2                   TAGGED_FOR_CACP              None   \n",
       "3  PATIENT_DECLINED_SERVICE_TO_IRIS         141965232   \n",
       "4         IRIS_OUTREACH_IN_PROGRESS         834172038   \n",
       "\n",
       "  IRIS_UNABLE_TO_SCHEDULE_REASON SECOND_WAVE_ENGAGEMENT_DATE  \\\n",
       "0                           None                        None   \n",
       "1                           None                        None   \n",
       "2                           None                        None   \n",
       "3                           None                        None   \n",
       "4                           None                  2024-04-11   \n",
       "\n",
       "                      PRAC_OR_LOCATION REFER_TO_PARTNER_BY  \\\n",
       "0                                 None           prac_name   \n",
       "1  West Cecil Health Center Inc (6261)           prac_name   \n",
       "2                                 None           prac_name   \n",
       "3    Adult Geriatrics of Wooster (902)           prac_name   \n",
       "4          Wolinsky Primary Care (942)           prac_name   \n",
       "\n",
       "  ORDERED_DATE_COL_NAMES ORDERED_DATES ORDERED_NORMALIZED_NUMS  \n",
       "0                     []            []                      []  \n",
       "1                     []            []                      []  \n",
       "2                     []            []                      []  \n",
       "3                     []            []                      []  \n",
       "4                     []            []                      []  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, pushing to snowflake\n",
      "pushed to snowflake\n",
      "--complete--\n",
      "\n",
      "--loading data--\n",
      "Number of rows: 429299\n",
      "Number of columns: 90\n",
      "--complete--\n",
      "\n",
      "--map_data_types_to_categories--\n",
      "upper-casing pandas columns before transform\n",
      "\n",
      "Mapping is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLUMN_NAME</th>\n",
       "      <th>NATIVE_DATA_TYPE</th>\n",
       "      <th>NATIVE_DATA_CLASS</th>\n",
       "      <th>TEST_DATA_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOCUMENT_COMPLETION_STATUS</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OPERATIONAL_ACO</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARKET</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CURRENT_PRACTICE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRAC_PRIMARY_FACILITY_TYPE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SECONDARY_REFERRAL_SOURCE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CONTRACT_PRAC_PRIMARY_FACILITY_TYPE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>REFERRAL_SOURCE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TARGETED_CONTRACT_MSSP_TRACK_GROUPED</td>\n",
       "      <td>StringType(19)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACO_DISPLAY_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CURRENT_CONTRACT_MSSP_TRACK_GROUPED</td>\n",
       "      <td>StringType(19)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TARGETED_MSSP_TRACK</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DO_NOT_CONTACT</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DNR</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PARTICIPATING_IN_CACP_SERVICE_SUB_STATUS</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CURRENT_PT_CONTRACT_TYPE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MRN</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>OPERATIONAL_MARKET_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OPERATIONAL_ACO_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SEX</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SECOND_WAVE_ENGAGEMENT_REASON</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CURRENT_MSSP_TRACK</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CONTRACT</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PRACTICE_ID</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PLANNING_SUMMARY</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RACE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MPOA</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PARTICIPATING_IN_CACP_SERVICE</td>\n",
       "      <td>StringType(3)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CONTRACT_AGREEMENT</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RE_REFERRAL_NOTES</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PATIENT_CONTRACT_TYPE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PRACTICE_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>WARM_HAND_OFF_METHOD</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>FQHC_FLAG</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>CONTRACT_PRAC_DISPLAY_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>DATE_OF_BIRTH</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LOCATION_DISPLAY_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>DOCUMENT_STATE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>PARTICIPATION_DISCONTINUED_SUB_REASON</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>CURRENT_PT_CONTRACT_TYPE_GROUPED</td>\n",
       "      <td>StringType(19)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>WAVE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>PRIMARY_NPI_DISPLAY_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>PREFERRED_NAME_CHANGE_SIGNIFICANCE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>PT_CONTRACT_TYPE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>MEMBER_ID</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ALEDADE_PT_ID</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>PRIMARY_NPI</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>LAST_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>FIRST_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>PRIMARY_FACILITY_TYPE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>PHONE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>START_OF_1ST_REGULAR____VIDA_ID</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>WARM_HAND_OFF_COMPLETED</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>CONTRACT_RISK_TYPE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>PRAC_DISPLAY_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>PT_CONTRACT_PAYER</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>COMBINED_LIVING_WILL_MPOA</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>CURRENT_PIPELINE_STEP</td>\n",
       "      <td>StringType(10)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>TREATMENT_PREFERENCES</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>PLANNING_SUMMARY_DATE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>PILOT_PARTICIPATION</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>DECLINED_REASON</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>PRIMARY_LANGUAGE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>WARM_HAND_OFF_COMPLETED_BY</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>HIDDEN_STATUS</td>\n",
       "      <td>StringType(23)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>CLINICIAN_NPI</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>PATIENT_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>COMBINED_LIVING_WILL_MPOA_DATE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>POLST</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>CLINICIAN_NAME</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>TARGETED_PT_CONTRACT_TYPE_GROUPED</td>\n",
       "      <td>StringType(19)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>LIVING_WILL</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>DNR_DATE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>ACO_CMS_ID</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>PARTICIPATION_DISCONTINUED_REASON</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>LIVING_WILL_DATE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>ZIP</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>MULTI_LOCATION_PRACTICE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>PATIENT_POPULATION</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>DECLINED_SUB_REASON</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>BENE_ID</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>CONTRACT_MSSP_TRACK</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>ETHNICITY</td>\n",
       "      <td>StringType(100)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>TARGETED_PT_CONTRACT_TYPE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>CURRENT_ENGAGEMENT_STATUS</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>CONTRACTED_ENTITY</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>IRIS_UNABLE_TO_SCHEDULE_REASON</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>SECOND_WAVE_ENGAGEMENT_DATE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>PRAC_OR_LOCATION</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>REFER_TO_PARTNER_BY</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 COLUMN_NAME      NATIVE_DATA_TYPE  \\\n",
       "0                 DOCUMENT_COMPLETION_STATUS  StringType(16777216)   \n",
       "1                            OPERATIONAL_ACO  StringType(16777216)   \n",
       "2                                     MARKET  StringType(16777216)   \n",
       "3                           CURRENT_PRACTICE  StringType(16777216)   \n",
       "4                 PRAC_PRIMARY_FACILITY_TYPE  StringType(16777216)   \n",
       "5                  SECONDARY_REFERRAL_SOURCE  StringType(16777216)   \n",
       "6        CONTRACT_PRAC_PRIMARY_FACILITY_TYPE  StringType(16777216)   \n",
       "7                            REFERRAL_SOURCE  StringType(16777216)   \n",
       "8       TARGETED_CONTRACT_MSSP_TRACK_GROUPED        StringType(19)   \n",
       "9                           ACO_DISPLAY_NAME  StringType(16777216)   \n",
       "10       CURRENT_CONTRACT_MSSP_TRACK_GROUPED        StringType(19)   \n",
       "11                       TARGETED_MSSP_TRACK  StringType(16777216)   \n",
       "12                            DO_NOT_CONTACT  StringType(16777216)   \n",
       "13                                       DNR  StringType(16777216)   \n",
       "14  PARTICIPATING_IN_CACP_SERVICE_SUB_STATUS  StringType(16777216)   \n",
       "15                  CURRENT_PT_CONTRACT_TYPE  StringType(16777216)   \n",
       "16                                       MRN  StringType(16777216)   \n",
       "17                   OPERATIONAL_MARKET_NAME  StringType(16777216)   \n",
       "18                      OPERATIONAL_ACO_NAME  StringType(16777216)   \n",
       "19                                       SEX  StringType(16777216)   \n",
       "20             SECOND_WAVE_ENGAGEMENT_REASON  StringType(16777216)   \n",
       "21                        CURRENT_MSSP_TRACK  StringType(16777216)   \n",
       "22                                  CONTRACT  StringType(16777216)   \n",
       "23                               PRACTICE_ID  StringType(16777216)   \n",
       "24                          PLANNING_SUMMARY  StringType(16777216)   \n",
       "25                                      RACE  StringType(16777216)   \n",
       "26                                      MPOA  StringType(16777216)   \n",
       "27             PARTICIPATING_IN_CACP_SERVICE         StringType(3)   \n",
       "28                        CONTRACT_AGREEMENT  StringType(16777216)   \n",
       "29                         RE_REFERRAL_NOTES  StringType(16777216)   \n",
       "30                     PATIENT_CONTRACT_TYPE  StringType(16777216)   \n",
       "31                             PRACTICE_NAME  StringType(16777216)   \n",
       "32                      WARM_HAND_OFF_METHOD  StringType(16777216)   \n",
       "33                                 FQHC_FLAG  StringType(16777216)   \n",
       "34                CONTRACT_PRAC_DISPLAY_NAME  StringType(16777216)   \n",
       "35                             DATE_OF_BIRTH  StringType(16777216)   \n",
       "36                     LOCATION_DISPLAY_NAME  StringType(16777216)   \n",
       "37                            DOCUMENT_STATE  StringType(16777216)   \n",
       "38     PARTICIPATION_DISCONTINUED_SUB_REASON  StringType(16777216)   \n",
       "39          CURRENT_PT_CONTRACT_TYPE_GROUPED        StringType(19)   \n",
       "40                                      WAVE  StringType(16777216)   \n",
       "41                  PRIMARY_NPI_DISPLAY_NAME  StringType(16777216)   \n",
       "42        PREFERRED_NAME_CHANGE_SIGNIFICANCE  StringType(16777216)   \n",
       "43                          PT_CONTRACT_TYPE  StringType(16777216)   \n",
       "44                                 MEMBER_ID  StringType(16777216)   \n",
       "45                             ALEDADE_PT_ID  StringType(16777216)   \n",
       "46                               PRIMARY_NPI  StringType(16777216)   \n",
       "47                                 LAST_NAME  StringType(16777216)   \n",
       "48                                FIRST_NAME  StringType(16777216)   \n",
       "49                     PRIMARY_FACILITY_TYPE  StringType(16777216)   \n",
       "50                                     PHONE  StringType(16777216)   \n",
       "51           START_OF_1ST_REGULAR____VIDA_ID  StringType(16777216)   \n",
       "52                   WARM_HAND_OFF_COMPLETED  StringType(16777216)   \n",
       "53                        CONTRACT_RISK_TYPE  StringType(16777216)   \n",
       "54                         PRAC_DISPLAY_NAME  StringType(16777216)   \n",
       "55                         PT_CONTRACT_PAYER  StringType(16777216)   \n",
       "56                 COMBINED_LIVING_WILL_MPOA  StringType(16777216)   \n",
       "57                     CURRENT_PIPELINE_STEP        StringType(10)   \n",
       "58                     TREATMENT_PREFERENCES  StringType(16777216)   \n",
       "59                     PLANNING_SUMMARY_DATE  StringType(16777216)   \n",
       "60                       PILOT_PARTICIPATION  StringType(16777216)   \n",
       "61                           DECLINED_REASON  StringType(16777216)   \n",
       "62                          PRIMARY_LANGUAGE  StringType(16777216)   \n",
       "63                WARM_HAND_OFF_COMPLETED_BY  StringType(16777216)   \n",
       "64                             HIDDEN_STATUS        StringType(23)   \n",
       "65                             CLINICIAN_NPI  StringType(16777216)   \n",
       "66                              PATIENT_NAME  StringType(16777216)   \n",
       "67            COMBINED_LIVING_WILL_MPOA_DATE  StringType(16777216)   \n",
       "68                                     POLST  StringType(16777216)   \n",
       "69                            CLINICIAN_NAME  StringType(16777216)   \n",
       "70         TARGETED_PT_CONTRACT_TYPE_GROUPED        StringType(19)   \n",
       "71                               LIVING_WILL  StringType(16777216)   \n",
       "72                                  DNR_DATE  StringType(16777216)   \n",
       "73                                ACO_CMS_ID  StringType(16777216)   \n",
       "74         PARTICIPATION_DISCONTINUED_REASON  StringType(16777216)   \n",
       "75                          LIVING_WILL_DATE  StringType(16777216)   \n",
       "76                                       ZIP  StringType(16777216)   \n",
       "77                   MULTI_LOCATION_PRACTICE  StringType(16777216)   \n",
       "78                        PATIENT_POPULATION  StringType(16777216)   \n",
       "79                       DECLINED_SUB_REASON  StringType(16777216)   \n",
       "80                                   BENE_ID  StringType(16777216)   \n",
       "81                       CONTRACT_MSSP_TRACK  StringType(16777216)   \n",
       "82                                 ETHNICITY       StringType(100)   \n",
       "83                 TARGETED_PT_CONTRACT_TYPE  StringType(16777216)   \n",
       "84                 CURRENT_ENGAGEMENT_STATUS  StringType(16777216)   \n",
       "85                         CONTRACTED_ENTITY  StringType(16777216)   \n",
       "86            IRIS_UNABLE_TO_SCHEDULE_REASON  StringType(16777216)   \n",
       "87               SECOND_WAVE_ENGAGEMENT_DATE  StringType(16777216)   \n",
       "88                          PRAC_OR_LOCATION  StringType(16777216)   \n",
       "89                       REFER_TO_PARTNER_BY  StringType(16777216)   \n",
       "\n",
       "   NATIVE_DATA_CLASS TEST_DATA_TYPE  \n",
       "0         StringType         STRING  \n",
       "1         StringType         STRING  \n",
       "2         StringType         STRING  \n",
       "3         StringType         STRING  \n",
       "4         StringType         STRING  \n",
       "5         StringType         STRING  \n",
       "6         StringType         STRING  \n",
       "7         StringType         STRING  \n",
       "8         StringType         STRING  \n",
       "9         StringType         STRING  \n",
       "10        StringType         STRING  \n",
       "11        StringType         STRING  \n",
       "12        StringType         STRING  \n",
       "13        StringType         STRING  \n",
       "14        StringType         STRING  \n",
       "15        StringType         STRING  \n",
       "16        StringType         STRING  \n",
       "17        StringType         STRING  \n",
       "18        StringType         STRING  \n",
       "19        StringType         STRING  \n",
       "20        StringType         STRING  \n",
       "21        StringType         STRING  \n",
       "22        StringType         STRING  \n",
       "23        StringType         STRING  \n",
       "24        StringType         STRING  \n",
       "25        StringType         STRING  \n",
       "26        StringType         STRING  \n",
       "27        StringType         STRING  \n",
       "28        StringType         STRING  \n",
       "29        StringType         STRING  \n",
       "30        StringType         STRING  \n",
       "31        StringType         STRING  \n",
       "32        StringType         STRING  \n",
       "33        StringType         STRING  \n",
       "34        StringType         STRING  \n",
       "35        StringType         STRING  \n",
       "36        StringType         STRING  \n",
       "37        StringType         STRING  \n",
       "38        StringType         STRING  \n",
       "39        StringType         STRING  \n",
       "40        StringType         STRING  \n",
       "41        StringType         STRING  \n",
       "42        StringType         STRING  \n",
       "43        StringType         STRING  \n",
       "44        StringType         STRING  \n",
       "45        StringType         STRING  \n",
       "46        StringType         STRING  \n",
       "47        StringType         STRING  \n",
       "48        StringType         STRING  \n",
       "49        StringType         STRING  \n",
       "50        StringType         STRING  \n",
       "51        StringType         STRING  \n",
       "52        StringType         STRING  \n",
       "53        StringType         STRING  \n",
       "54        StringType         STRING  \n",
       "55        StringType         STRING  \n",
       "56        StringType         STRING  \n",
       "57        StringType         STRING  \n",
       "58        StringType         STRING  \n",
       "59        StringType         STRING  \n",
       "60        StringType         STRING  \n",
       "61        StringType         STRING  \n",
       "62        StringType         STRING  \n",
       "63        StringType         STRING  \n",
       "64        StringType         STRING  \n",
       "65        StringType         STRING  \n",
       "66        StringType         STRING  \n",
       "67        StringType         STRING  \n",
       "68        StringType         STRING  \n",
       "69        StringType         STRING  \n",
       "70        StringType         STRING  \n",
       "71        StringType         STRING  \n",
       "72        StringType         STRING  \n",
       "73        StringType         STRING  \n",
       "74        StringType         STRING  \n",
       "75        StringType         STRING  \n",
       "76        StringType         STRING  \n",
       "77        StringType         STRING  \n",
       "78        StringType         STRING  \n",
       "79        StringType         STRING  \n",
       "80        StringType         STRING  \n",
       "81        StringType         STRING  \n",
       "82        StringType         STRING  \n",
       "83        StringType         STRING  \n",
       "84        StringType         STRING  \n",
       "85        StringType         STRING  \n",
       "86        StringType         STRING  \n",
       "87        StringType         STRING  \n",
       "88        StringType         STRING  \n",
       "89        StringType         STRING  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--complete--\n",
      "\n",
      "--run_column_analysis--\n",
      "No subset provided - using original raw snowflake data\n",
      "----------starting testing----------\n",
      "Copy schema_mapped_df with COLUMN_NAME as index for results compilation\n",
      "No columns for test_data_type 'DATE' found.\n",
      "No columns for test_data_type 'NUMBER' found.\n",
      "No columns for test_data_type 'BINARY' found.\n",
      "No columns for test_data_type 'VARIANT' found.\n",
      "No columns for test_data_type 'GEOGRAPHY' found.\n",
      "No columns for test_data_type 'CATEGORICAL' found.\n",
      "Testing data type:  GENERAL\n",
      "raw_data_subset:\n",
      "   NULL_PERCENTAGE...\n",
      "   DISTINCT_VALUES...\n",
      "distinct done.\n",
      "Testing data type:  STRING\n",
      "   STRING_AVG_LENGTH...\n",
      "   STRING_MAX_LENGTH...\n",
      "   STRING_MIN_LENGTH...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLUMN_NAME</th>\n",
       "      <th>NATIVE_DATA_TYPE</th>\n",
       "      <th>NATIVE_DATA_CLASS</th>\n",
       "      <th>TEST_DATA_TYPE</th>\n",
       "      <th>NULL_PERCENTAGE</th>\n",
       "      <th>DISTINCT_VALUES</th>\n",
       "      <th>DISTINCT_COUNT</th>\n",
       "      <th>STRING_AVG_LENGTH</th>\n",
       "      <th>STRING_MAX_LENGTH</th>\n",
       "      <th>STRING_MIN_LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOCUMENT_COMPLETION_STATUS</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OPERATIONAL_ACO</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>3.6513</td>\n",
       "      <td>[\"TN Opportunity ACO\", \"NJ Multi-State--AAC 10...</td>\n",
       "      <td>124</td>\n",
       "      <td>16.521353</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARKET</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>3.6816</td>\n",
       "      <td>[\"Kentucky\", \"Alabama\", \"Upper Midwest (IL, IN...</td>\n",
       "      <td>31</td>\n",
       "      <td>11.334215</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CURRENT_PRACTICE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>[\"2174\", \"2247\", \"512\", \"1228\", \"140\", \"1421\",...</td>\n",
       "      <td>1965</td>\n",
       "      <td>3.328310</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRAC_PRIMARY_FACILITY_TYPE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>11.2879</td>\n",
       "      <td>[\"CAH\", \" Unknown Facility Type\", \"Provider-ba...</td>\n",
       "      <td>6</td>\n",
       "      <td>14.300572</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>CONTRACTED_ENTITY</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>36.5251</td>\n",
       "      <td>[\"956419205\", \"522010253\", \"800682402\", \"56099...</td>\n",
       "      <td>1853</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>IRIS_UNABLE_TO_SCHEDULE_REASON</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>81.0393</td>\n",
       "      <td>[\"NO_LONGER_WITH_PHYSICIAN\", \"DECEASED\", \"PATI...</td>\n",
       "      <td>8</td>\n",
       "      <td>12.330266</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>SECOND_WAVE_ENGAGEMENT_DATE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>88.0370</td>\n",
       "      <td>[\"2022-09-12\", \"2023-03-31\", \"2024-02-16\", \"20...</td>\n",
       "      <td>41</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>PRAC_OR_LOCATION</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>11.2879</td>\n",
       "      <td>[\"Dundalk (6156)\", \"Bear River Medical Arts PC...</td>\n",
       "      <td>4400</td>\n",
       "      <td>31.353836</td>\n",
       "      <td>90.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>REFER_TO_PARTNER_BY</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>[\"pcp_name\", \"prac_name\", \"loc_name\", \"prov_na...</td>\n",
       "      <td>4</td>\n",
       "      <td>8.999991</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       COLUMN_NAME      NATIVE_DATA_TYPE NATIVE_DATA_CLASS  \\\n",
       "0       DOCUMENT_COMPLETION_STATUS  StringType(16777216)        StringType   \n",
       "1                  OPERATIONAL_ACO  StringType(16777216)        StringType   \n",
       "2                           MARKET  StringType(16777216)        StringType   \n",
       "3                 CURRENT_PRACTICE  StringType(16777216)        StringType   \n",
       "4       PRAC_PRIMARY_FACILITY_TYPE  StringType(16777216)        StringType   \n",
       "..                             ...                   ...               ...   \n",
       "85               CONTRACTED_ENTITY  StringType(16777216)        StringType   \n",
       "86  IRIS_UNABLE_TO_SCHEDULE_REASON  StringType(16777216)        StringType   \n",
       "87     SECOND_WAVE_ENGAGEMENT_DATE  StringType(16777216)        StringType   \n",
       "88                PRAC_OR_LOCATION  StringType(16777216)        StringType   \n",
       "89             REFER_TO_PARTNER_BY  StringType(16777216)        StringType   \n",
       "\n",
       "   TEST_DATA_TYPE  NULL_PERCENTAGE  \\\n",
       "0          STRING         100.0000   \n",
       "1          STRING           3.6513   \n",
       "2          STRING           3.6816   \n",
       "3          STRING           0.0002   \n",
       "4          STRING          11.2879   \n",
       "..            ...              ...   \n",
       "85         STRING          36.5251   \n",
       "86         STRING          81.0393   \n",
       "87         STRING          88.0370   \n",
       "88         STRING          11.2879   \n",
       "89         STRING           0.0158   \n",
       "\n",
       "                                      DISTINCT_VALUES  DISTINCT_COUNT  \\\n",
       "0                                                  []               0   \n",
       "1   [\"TN Opportunity ACO\", \"NJ Multi-State--AAC 10...             124   \n",
       "2   [\"Kentucky\", \"Alabama\", \"Upper Midwest (IL, IN...              31   \n",
       "3   [\"2174\", \"2247\", \"512\", \"1228\", \"140\", \"1421\",...            1965   \n",
       "4   [\"CAH\", \" Unknown Facility Type\", \"Provider-ba...               6   \n",
       "..                                                ...             ...   \n",
       "85  [\"956419205\", \"522010253\", \"800682402\", \"56099...            1853   \n",
       "86  [\"NO_LONGER_WITH_PHYSICIAN\", \"DECEASED\", \"PATI...               8   \n",
       "87  [\"2022-09-12\", \"2023-03-31\", \"2024-02-16\", \"20...              41   \n",
       "88  [\"Dundalk (6156)\", \"Bear River Medical Arts PC...            4400   \n",
       "89  [\"pcp_name\", \"prac_name\", \"loc_name\", \"prov_na...               4   \n",
       "\n",
       "    STRING_AVG_LENGTH  STRING_MAX_LENGTH  STRING_MIN_LENGTH  \n",
       "0                 NaN                NaN                NaN  \n",
       "1           16.521353               41.0                6.0  \n",
       "2           11.334215               36.0                4.0  \n",
       "3            3.328310                4.0                1.0  \n",
       "4           14.300572               22.0                3.0  \n",
       "..                ...                ...                ...  \n",
       "85           9.000000                9.0                9.0  \n",
       "86          12.330266               38.0                8.0  \n",
       "87          10.000000               10.0               10.0  \n",
       "88          31.353836               90.0               10.0  \n",
       "89           8.999991                9.0                8.0  \n",
       "\n",
       "[90 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--complete--\n",
      "\n",
      "--uploading analysis to snowflake--\n",
      "#-----get data from before merge-----\n",
      "\n",
      "-----dropping old temp table, and making new-----\n",
      "\n",
      "#-----add missing columns-----\n",
      "\n",
      "#-----perform merge-----\n",
      "\n",
      "#-----display what was uploaded:-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLUMN_NAME</th>\n",
       "      <th>NATIVE_DATA_TYPE</th>\n",
       "      <th>NATIVE_DATA_CLASS</th>\n",
       "      <th>TEST_DATA_TYPE</th>\n",
       "      <th>NULL_PERCENTAGE</th>\n",
       "      <th>DISTINCT_VALUES</th>\n",
       "      <th>DISTINCT_COUNT</th>\n",
       "      <th>STRING_AVG_LENGTH</th>\n",
       "      <th>STRING_MAX_LENGTH</th>\n",
       "      <th>STRING_MIN_LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OPERATIONAL_MARKET_NAME_NORMALIZED</td>\n",
       "      <td>LongType()</td>\n",
       "      <td>LongType</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CONTRACT_AGREEMENT_NORMALIZED</td>\n",
       "      <td>LongType()</td>\n",
       "      <td>LongType</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>START_OF_1ST_REGULAR____VIDA_ID_NORMALIZED</td>\n",
       "      <td>LongType()</td>\n",
       "      <td>LongType</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SECOND_WAVE_ENGAGEMENT_DATE_NORMALIZED</td>\n",
       "      <td>LongType()</td>\n",
       "      <td>LongType</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IS_CURRENTLY_HIPRI_NORMALIZED</td>\n",
       "      <td>LongType()</td>\n",
       "      <td>LongType</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>PT_CONTRACT_PAYER</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>11.2879</td>\n",
       "      <td>[\"Allwell MA\", \"AmeriHealth DE\", \"UHC\", \"BCBS ...</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>7.647881</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>CONTRACT_RISK_TYPE</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>11.2879</td>\n",
       "      <td>[\"\", \"1SR\", \"2SR\"]</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>2.127878</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>CONTRACT_MSSP_TRACK</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>11.2879</td>\n",
       "      <td>[\"Basic E\", \"Enhanced\", \"\", \"Global\"]</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>5.146791</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>MEMBER_ID</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>11.441</td>\n",
       "      <td>[\"2YE2CX7QD14\", \"MEN101558799100\", \"3V81FK3RU3...</td>\n",
       "      <td>380177.00000</td>\n",
       "      <td>11.070813</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>BENE_ID</td>\n",
       "      <td>StringType(16777216)</td>\n",
       "      <td>StringType</td>\n",
       "      <td>STRING</td>\n",
       "      <td>11.2879</td>\n",
       "      <td>[\"17043552\", \"35472287\", \"2950275\", \"6344868\",...</td>\n",
       "      <td>380840.00000</td>\n",
       "      <td>7.594229</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COLUMN_NAME      NATIVE_DATA_TYPE  \\\n",
       "0            OPERATIONAL_MARKET_NAME_NORMALIZED            LongType()   \n",
       "1                 CONTRACT_AGREEMENT_NORMALIZED            LongType()   \n",
       "2    START_OF_1ST_REGULAR____VIDA_ID_NORMALIZED            LongType()   \n",
       "3        SECOND_WAVE_ENGAGEMENT_DATE_NORMALIZED            LongType()   \n",
       "4                 IS_CURRENTLY_HIPRI_NORMALIZED            LongType()   \n",
       "..                                          ...                   ...   \n",
       "208                           PT_CONTRACT_PAYER  StringType(16777216)   \n",
       "209                          CONTRACT_RISK_TYPE  StringType(16777216)   \n",
       "210                         CONTRACT_MSSP_TRACK  StringType(16777216)   \n",
       "211                                   MEMBER_ID  StringType(16777216)   \n",
       "212                                     BENE_ID  StringType(16777216)   \n",
       "\n",
       "    NATIVE_DATA_CLASS TEST_DATA_TYPE NULL_PERCENTAGE  \\\n",
       "0            LongType         NUMBER            None   \n",
       "1            LongType         NUMBER            None   \n",
       "2            LongType         NUMBER            None   \n",
       "3            LongType         NUMBER            None   \n",
       "4            LongType         NUMBER            None   \n",
       "..                ...            ...             ...   \n",
       "208        StringType         STRING         11.2879   \n",
       "209        StringType         STRING         11.2879   \n",
       "210        StringType         STRING         11.2879   \n",
       "211        StringType         STRING          11.441   \n",
       "212        StringType         STRING         11.2879   \n",
       "\n",
       "                                       DISTINCT_VALUES DISTINCT_COUNT  \\\n",
       "0                                                 None           None   \n",
       "1                                                 None           None   \n",
       "2                                                 None           None   \n",
       "3                                                 None           None   \n",
       "4                                                 None           None   \n",
       "..                                                 ...            ...   \n",
       "208  [\"Allwell MA\", \"AmeriHealth DE\", \"UHC\", \"BCBS ...       26.00000   \n",
       "209                                 [\"\", \"1SR\", \"2SR\"]        3.00000   \n",
       "210              [\"Basic E\", \"Enhanced\", \"\", \"Global\"]        4.00000   \n",
       "211  [\"2YE2CX7QD14\", \"MEN101558799100\", \"3V81FK3RU3...   380177.00000   \n",
       "212  [\"17043552\", \"35472287\", \"2950275\", \"6344868\",...   380840.00000   \n",
       "\n",
       "    STRING_AVG_LENGTH STRING_MAX_LENGTH STRING_MIN_LENGTH  \n",
       "0                None              None              None  \n",
       "1                None              None              None  \n",
       "2                None              None              None  \n",
       "3                None              None              None  \n",
       "4                None              None              None  \n",
       "..                ...               ...               ...  \n",
       "208          7.647881                20                 3  \n",
       "209          2.127878                 3                 0  \n",
       "210          5.146791                 8                 0  \n",
       "211         11.070813                16                 5  \n",
       "212          7.594229                 8                 5  \n",
       "\n",
       "[213 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------------------------drop temp table------------------------------\n",
      "--complete--\n",
      "\n",
      "--classify columns for corr/anova--\n",
      "Starting...\n",
      "Columns removed from testing:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLUMN_NAME</th>\n",
       "      <th>NULL_PERCENTAGE</th>\n",
       "      <th>DISTINCT_VALUES</th>\n",
       "      <th>DISTINCT_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOCUMENT_COMPLETION_STATUS</td>\n",
       "      <td>100.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  COLUMN_NAME  NULL_PERCENTAGE DISTINCT_VALUES  DISTINCT_COUNT\n",
       "0  DOCUMENT_COMPLETION_STATUS            100.0              []               0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "# of columns in original:  90\n",
      "# of columns to test:  89\n",
      "# of numeric columns to test:  0\n",
      "# of categorical columns to test:  89\n",
      "--complete--\n",
      "\n",
      "--convert dates to unix encoding--\n",
      "#--------------------------    Convert Dates to Unix #s for Encoding (snowpark)     ----------------------------\n",
      "Convert date columns to Unix time in seconds (epoch time)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--complete--\n",
      "\n",
      "--sort category values by goal metric--\n",
      "#--------------------------    Sort Category Values by Avg Goal/Value Metric     ----------------------------\n"
     ]
    },
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1304): 01b4599b-0604-bff1-0014-1f8374606a0e: 000904 (42000): SQL compilation error: error line 1 at position 30\ninvalid identifier 'CURRENT_VALUE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 114\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m#clear_output(wait=False)\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m#Category analysis:\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m#...\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaster Controller Complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 114\u001b[0m \u001b[43mmaster_controller\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 73\u001b[0m, in \u001b[0;36mmaster_controller\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m#clear_output(wait=False)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--sort category values by goal metric--\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msort_category_values_by_goal_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--complete--\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 51\u001b[0m, in \u001b[0;36msort_category_values_by_goal_metric\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_output_with_ranking\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Execute the function and print debugging information\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m sorted_category_averages \u001b[38;5;241m=\u001b[39m \u001b[43mdynamic_sort_by_avg_current_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data_snowpark_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCATEGORICAL_COLUMN_NAMES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample output:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(sorted_category_averages[\u001b[38;5;28mlist\u001b[39m(sorted_category_averages\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[16], line 37\u001b[0m, in \u001b[0;36msort_category_values_by_goal_metric.<locals>.dynamic_sort_by_avg_current_value\u001b[1;34m(df, categorical_columns)\u001b[0m\n\u001b[0;32m     34\u001b[0m df_grouped_avg \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupBy(col(column))\u001b[38;5;241m.\u001b[39magg(avg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_current_value\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Sort the result by the average CURRENT_VALUE in descending order\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m df_sorted_avg \u001b[38;5;241m=\u001b[39m \u001b[43mdf_grouped_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_current_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Collect the results\u001b[39;00m\n\u001b[0;32m     40\u001b[0m sorted_category_avg[column] \u001b[38;5;241m=\u001b[39m df_sorted_avg\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\_internal\\telemetry.py:184\u001b[0m, in \u001b[0;36mdf_api_usage.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 184\u001b[0m     r \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    185\u001b[0m     plan \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39m_select_statement \u001b[38;5;129;01mor\u001b[39;00m r\u001b[38;5;241m.\u001b[39m_plan\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# Some DataFrame APIs call other DataFrame APIs, so we need to remove the extra call\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\dataframe.py:1326\u001b[0m, in \u001b[0;36mDataFrame.sort\u001b[1;34m(self, ascending, *cols)\u001b[0m\n\u001b[0;32m   1321\u001b[0m         sort_exprs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m   1322\u001b[0m             SortOrder(exprs[idx], orders[idx] \u001b[38;5;28;01mif\u001b[39;00m orders \u001b[38;5;28;01melse\u001b[39;00m Ascending())\n\u001b[0;32m   1323\u001b[0m         )\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_statement:\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_plan(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_statement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43msort_exprs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_plan(Sort(sort_exprs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\select_statement.py:719\u001b[0m, in \u001b[0;36mSelectStatement.sort\u001b[1;34m(self, cols)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msort\u001b[39m(\u001b[38;5;28mself\u001b[39m, cols: List[Expression]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelectStatement\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    716\u001b[0m     can_be_flattened \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    717\u001b[0m         (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_disabled)\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m can_clause_dependent_columns_flatten(\n\u001b[1;32m--> 719\u001b[0m             derive_dependent_columns(\u001b[38;5;241m*\u001b[39mcols), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_states\u001b[49m\n\u001b[0;32m    720\u001b[0m         )\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_data_generator_exp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection)\n\u001b[0;32m    722\u001b[0m     )\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m can_be_flattened:\n\u001b[0;32m    724\u001b[0m         new \u001b[38;5;241m=\u001b[39m copy(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\select_statement.py:477\u001b[0m, in \u001b[0;36mSelectStatement.column_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_clause:\n\u001b[1;32m--> 477\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_states\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcolumn_states  \u001b[38;5;66;03m# will assign value to self._column_states\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\select_statement.py:270\u001b[0m, in \u001b[0;36mSelectable.column_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A dictionary that contains the column states of a query.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03mRefer to class ColumnStateDict.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_states \u001b[38;5;241m=\u001b[39m initiate_column_states(\n\u001b[1;32m--> 270\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msnowflake_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattributes\u001b[49m,\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer,\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_aliased_col_name_to_real_col_name,\n\u001b[0;32m    273\u001b[0m     )\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_states\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\functools.py:993\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    991\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 993\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    995\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:255\u001b[0m, in \u001b[0;36mSnowflakePlan.attributes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattributes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Attribute]:\n\u001b[1;32m--> 255\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# No simplifier case relies on this schema_query change to update SHOW TABLES to a nested sql friendly query.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_query \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39msql_simplifier_enabled:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\schema_utils.py:91\u001b[0m, in \u001b[0;36manalyze_attributes\u001b[1;34m(sql, session)\u001b[0m\n\u001b[0;32m     88\u001b[0m     session\u001b[38;5;241m.\u001b[39m_run_query(sql)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_result_meta_to_attribute(session\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39m_cursor\u001b[38;5;241m.\u001b[39mdescription)\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_result_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\session.py:2011\u001b[0m, in \u001b[0;36mSession._get_result_attributes\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_result_attributes\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Attribute]:\n\u001b[1;32m-> 2011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:180\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m         ne \u001b[38;5;241m=\u001b[39m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[0;32m    178\u001b[0m             e\n\u001b[0;32m    179\u001b[0m         )\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ne\u001b[38;5;241m.\u001b[39mwith_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     ne \u001b[38;5;241m=\u001b[39m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[0;32m    183\u001b[0m         e\n\u001b[0;32m    184\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:116\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mProgrammingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    118\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:233\u001b[0m, in \u001b[0;36mServerConnection.get_result_attributes\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;129m@SnowflakePlan\u001b[39m\u001b[38;5;241m.\u001b[39mDecorator\u001b[38;5;241m.\u001b[39mwrap_exception\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_result_attributes\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Attribute]:\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_result_meta_to_attribute(\u001b[43mrun_new_describe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\schema_utils.py:144\u001b[0m, in \u001b[0;36mrun_new_describe\u001b[1;34m(cursor, query)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# ResultMetadataV2 may not currently be a type, depending on the connector\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# version, so the argument types are pyright ignored\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cursor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_describe_internal\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# Pyright does not perform narrowing here\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_describe_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescribe(query)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\connector\\cursor.py:1176\u001b[0m, in \u001b[0;36mSnowflakeCursor._describe_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Obtain the schema of the result without executing the query.\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m \n\u001b[0;32m   1167\u001b[0m \u001b[38;5;124;03mThis function takes the same arguments as execute, please refer to that function\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;124;03m    The schema of the result, in the new result metadata format.\u001b[39;00m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_describe_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_internal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_description\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\connector\\cursor.py:1136\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[1;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1133\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1134\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[1;32m-> 1136\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\connector\\errors.py:290\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[0;32m    269\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[0;32m    298\u001b[0m             error_class,\n\u001b[0;32m    299\u001b[0m             error_value,\n\u001b[0;32m    300\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\connector\\errors.py:345\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[1;32m--> 345\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\snowflake\\connector\\errors.py:221\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    219\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[0;32m    222\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    223\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[0;32m    224\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    225\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    226\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    227\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[0;32m    229\u001b[0m     ),\n\u001b[0;32m    230\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[0;32m    231\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[0;32m    232\u001b[0m )\n",
      "\u001b[1;31mSnowparkSQLException\u001b[0m: (1304): 01b4599b-0604-bff1-0014-1f8374606a0e: 000904 (42000): SQL compilation error: error line 1 at position 30\ninvalid identifier 'CURRENT_VALUE'"
     ]
    }
   ],
   "source": [
    "#------------------------ Master Controller (Run all Functions) -------------------------------------\n",
    "def master_controller ():\n",
    "    #Collapse all cells: Ctrl + Shift + Left Arrow\n",
    "    #Uncollapse all cells: Ctrl + Shift + Right Arrow\n",
    "\n",
    "    print(\"Master Controller Starting..\")\n",
    "    \n",
    "    #General:\n",
    "    print(\"--Getting user inputs/creating session--\")\n",
    "    get_user_inputs()\n",
    "    get_or_create_session()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "\n",
    "    print(\"Converting Date Fields...\")  \n",
    "    convert_potential_date_fields()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"--Adding Normalized Dates--\")  \n",
    "    add_normalized_dates()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "\n",
    "    print(\"--Adding date sequences--\")\n",
    "    add_date_sequences()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "\n",
    "    print(\"--loading data--\")\n",
    "    #load_data(columns_to_load = None, num_rows = 5000)\n",
    "    load_data()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "    \n",
    "    print(\"--map_data_types_to_categories--\")\n",
    "    map_data_types_to_categories()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "\n",
    "    print(\"--run_column_analysis--\")\n",
    "    run_column_analysis()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "    '''apply the \"Fix true/false function\" to the categorical values upstream, instead of just the count'''\n",
    "\n",
    "    print(\"--uploading analysis to snowflake--\")\n",
    "    upload_column_analysis_to_snowflake()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "\n",
    "    '''Encoding:'''\n",
    "    print(\"--classify columns for corr/anova--\")\n",
    "    classify_columns_for_corr_and_anova()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "\n",
    "    print(\"--convert dates to unix encoding--\")\n",
    "    convert_dates_to_unix_encoding()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "\n",
    "    print(\"--sort category values by goal metric--\")\n",
    "    sort_category_values_by_goal_metric()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "    \n",
    "    print(\"--convert category values to encoding--\")\n",
    "    convert_category_values_to_encoding()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "\n",
    "    '''correlation analyis:'''\n",
    "    print(\"--run standard encoded corr analysis--\")\n",
    "    run_standard_encoded_corr_analysis()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "    \n",
    "    print(\"--run binary encoded corr analysis--\")\n",
    "    run_binary_encoded_corr_analysis()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "\n",
    "    print(\"--impute nulls--\")\n",
    "    impute_nulls()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "    \n",
    "    print(\"--calculate MLR--\")\n",
    "    calculate_MLR()\n",
    "    print(\"--complete--\")\n",
    "    print(\"\")\n",
    "    #clear_output(wait=False)\n",
    "\n",
    "    #Category analysis:\n",
    "    #...\n",
    "\n",
    "    print(\"Master Controller Complete.\")\n",
    "\n",
    "master_controller()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644a581-a84b-4b9c-b2e4-2e31f60e1e62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34af78-e07d-4bbd-abe7-c63b4dd0db30",
   "metadata": {},
   "source": [
    "#### Parking Lot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed9161f1-414e-48ba-a942-224708edc6fd",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#paused - UDF tests, ignoring...\n",
    "from snowflake.snowpark.types import StringType\n",
    "from snowflake.snowpark.functions import udf\n",
    "\n",
    "# Define a simple UDF that returns a string message\n",
    "@udf(return_type=StringType())\n",
    "def simple_udf_example():\n",
    "    return \"Hello from Snowpark UDF!\"\n",
    "\n",
    "# Register the UDF in the current session - Corrected Approach\n",
    "session.udf.register(\n",
    "    func=simple_udf_example,\n",
    "    name=\"simple_udf_example\",\n",
    "    return_type=StringType(),\n",
    "    is_permanent=False,\n",
    "    replace=True\n",
    ")\n",
    "\n",
    "# Test the UDF\n",
    "result = session.sql(\"SELECT simple_udf_example()\").collect()\n",
    "print(result[0][0])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d480956-5913-4d72-a52c-849934d5eb72",
   "metadata": {
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#paused - failed attempt to caterogy/value sort...\n",
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark.types import IntegerType\n",
    "'''\n",
    "add_one = sproc(lambda session, x: session.sql(f\"select {x} + 1\").collect()[0][0],\n",
    "    return_type=IntegerType(), input_types=[IntegerType()], name=\"my_sproc\", replace=True, packages=[\"snowflake-snowpark-python\"])\n",
    "\n",
    "print(add_one(1))\n",
    "\n",
    "# Define a stored procedure that performs a hypothetical operation on categorical data\n",
    "calculate_category_averages = sproc(\n",
    "    lambda session, table_name, column_name: session.sql(f\"\"\"\n",
    "        SELECT COUNT(DISTINCT {column_name}) \n",
    "        FROM {table_name}\n",
    "    \"\"\").collect()[0][0],\n",
    "    return_type=IntegerType(),\n",
    "    input_types=[StringType(), StringType()],\n",
    "    name=\"calculate_category_averages\",\n",
    "    replace=True,\n",
    "    packages=[\"snowflake-snowpark-python\"]\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "# Note: Replace 'your_table_name' and 'your_column_name' with actual table and column names\n",
    "number_of_unique_categories = calculate_category_averages('BI_ENROLL_RATE_PROD_DATA', 'CURRENT_VALUE')\n",
    "print(number_of_unique_categories)\n",
    "'''\n",
    "\n",
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark.types import IntegerType, StringType, DataFrameType\n",
    "\n",
    "# Stored Procedure for counting distinct values in a specific column of the 'BI_ENROLL_RATE_FULL_2' table\n",
    "count_distinct_values = sproc(\n",
    "    lambda session, column_name: session.sql(f\"\"\"\n",
    "        SELECT COUNT(DISTINCT {column_name}) \n",
    "        FROM EDW_PROD.ANALYTICS.BI_ENROLL_RATE_FULL_2\n",
    "    \"\"\").collect()[0][0],\n",
    "    return_type=IntegerType(),\n",
    "    input_types=[StringType()],\n",
    "    name=\"count_distinct_values\",\n",
    "    replace=True,\n",
    "    packages=[\"snowflake-snowpark-python\"]\n",
    ")\n",
    "\n",
    "# Example usage for counting distinct values\n",
    "number_of_unique_categories = count_distinct_values('YOUR_COLUMN_NAME_HERE')\n",
    "print(f\"Number of unique categories: {number_of_unique_categories}\")\n",
    "\n",
    "# Stored Procedure for retrieving condensed analysis stats from the 'ENROLL_RATE_COLUMN_ANALYSIS' table\n",
    "get_condensed_analysis = sproc(\n",
    "    lambda session: session.table(\"EDW_PROD.ANALYTICS.ENROLL_RATE_COLUMN_ANALYSIS\"),\n",
    "    return_type=DataFrameType(),\n",
    "    name=\"get_condensed_analysis\",\n",
    "    replace=True,\n",
    "    packages=[\"snowflake-snowpark-python\"]\n",
    ")\n",
    "\n",
    "# Example usage for retrieving condensed analysis\n",
    "condensed_analysis_df = get_condensed_analysis()\n",
    "condensed_analysis_df.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c223b51-9526-404c-ace7-aaece44aba19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "#paused - failed attempt to label-encode\n",
    "print(\"starting...\")\n",
    "#showall(raw_categorical_sorted, 1000, 200)\n",
    "\n",
    "#showall(categorical_test_results)\n",
    "\n",
    "#print(CATEGORICAL_COLUMN_NAMES)\n",
    "\n",
    "# Filtering 'categorical_test_results' to only columns \"COLUMN_NAME\" and \"DISTINCT_VALUES\"\n",
    "#categorical_test_results_filtered = filtered_test_results_df[[\"COLUMN_NAME\", \"DISTINCT_VALUES\"]]\n",
    "\n",
    "# Further filter 'categorical_test_results_filtered' to only rows where \"COLUMN_NAME\" is in 'CATEGORICAL_COLUMN_NAMES'\n",
    "#categorical_test_results_filtered = categorical_test_results_filtered[\n",
    "#    categorical_test_results_filtered[\"COLUMN_NAME\"].isin(CATEGORICAL_COLUMN_NAMES)\n",
    "#]\n",
    "\n",
    "#showall(categorical_test_results_filtered)\n",
    "\n",
    "# Create the new DataFrame with the correct structure\n",
    "#new_df_structured = pd.DataFrame(columns=categorical_test_results_filtered['COLUMN_NAME'])\n",
    "\n",
    "# Convert the string representations to actual lists\n",
    "#filtered_test_results_df['DISTINCT_VALUES'] = filtered_test_results_df['DISTINCT_VALUES'].apply(json.loads)\n",
    "\n",
    "#display(filtered_test_results_df)\n",
    "\n",
    "from snowflake.ml.modeling.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 2: Clone the DataFrame\n",
    "enroll_stable_copy_cloned = enroll_stable_copy.select(\"*\")\n",
    "\n",
    "# Step 3: Apply Label Encoding separately\n",
    "for col_name in CATEGORICAL_COLUMN_NAMES:\n",
    "    print(col_name)\n",
    "    # Initialize LabelEncoder for the current column\n",
    "    label_encoder = LabelEncoder(input_cols=col_name, output_cols=f\"{col_name}_encoded\")\n",
    "    print(label_encoder)\n",
    "    # Fit the LabelEncoder\n",
    "    label_encoder = label_encoder.fit(enroll_stable_copy_cloned)\n",
    "    print(label_encoder)   \n",
    "    # Transform the data\n",
    "    enroll_stable_copy_cloned = label_encoder.transform(enroll_stable_copy_cloned)\n",
    "    \n",
    "temp_print = enroll_stable_copy_cloned.limit(5)\n",
    "temp_print.show()\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14041a-1022-44b3-8b8b-2066f362af8b",
   "metadata": {},
   "source": [
    "#### Documentation/To-Do-List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798865d6-e140-4447-8e6d-868728a75cc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------Documentation-------------------------------------\n",
    "'''\n",
    "\n",
    "#----List of functions ... ----\n",
    "#General:\n",
    "    #get_user_inputs\n",
    "    #get_or_create_session\n",
    "    #load_data\n",
    "#Column analysis:\n",
    "    #map_data_types_to_categories\n",
    "    #run_column_analysis\n",
    "    #upload_column_tests_to_snowflake\n",
    "#Master Controller:\n",
    "    #master_controller\n",
    "\n",
    "\n",
    "\n",
    "#---- Structure/template... ----\n",
    "#Initialize Parameters\n",
    "    #This section initializes all parameters that will be used\n",
    "    #Parameter = a variable that will be referenced/updated outside of the function that sets its initial value\n",
    "    #Below is a list of all functions in this script, and which variables that function sets\n",
    "    \n",
    "    #Structured like so...\n",
    "    #--name_of_function--\n",
    "    #1st_param_it_sets\n",
    "    #2nd_param_it_sets\n",
    "    #...\n",
    "\n",
    "#Functions\n",
    "    #All functions will be structured like so...\n",
    "    \n",
    "    #------------------------func_name-------------------------------------\n",
    "    \n",
    "    #def func_name():\n",
    "        #__Needed from previous functions__\n",
    "        #--name_of_function_1--\n",
    "        #1st_param\n",
    "        #2nd_param\n",
    "    \n",
    "        #--name_of_function_2--\n",
    "        #1st_param\n",
    "        #2nd_param\n",
    "    \n",
    "        #...\n",
    "    \n",
    "        #__Set for the first time in this function__\n",
    "        #--name_of_function_1--\n",
    "        #1st_param\n",
    "        #2nd_param\n",
    "    \n",
    "        #--name_of_function_2--\n",
    "        #1st_param\n",
    "        #2nd_param\n",
    "    \n",
    "        #...\n",
    "    \n",
    "        #___function logic starts here___\n",
    "        #function logic...\n",
    "        ''';"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acbae111-6221-470a-bf72-5308aa2a1f1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Initialize an empty list for new rows\n",
    "new_rows = []\n",
    "top_num = 10\n",
    "\n",
    "# Identify all 'percent_change_in_...' columns in the DataFrame\n",
    "percent_change_columns = [col for col in stats_df.columns if col.startswith('percent_change_in_')]\n",
    "\n",
    "# For each row in the DataFrame\n",
    "for index, row in stats_df.iterrows():\n",
    "    # For each 'percent_change_in_...' column\n",
    "    for col in percent_change_columns:\n",
    "        # Extract measure name and comparator from the column name\n",
    "        parts = col.split('_compared_to_')\n",
    "        measure = parts[0].replace('percent_change_in_', '')\n",
    "        comparator = parts[1]\n",
    "        \n",
    "        # The dictionary of changes\n",
    "        changes_dict = row[col]\n",
    "        \n",
    "        # Transform dictionary entries into a list of tuples (column_name, percent_change)\n",
    "        changes_list = [(column_name, details['percent_change']) for column_name, details in changes_dict.items()]\n",
    "        \n",
    "        # Sort the list by percent_change\n",
    "        changes_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Extract top 10 positive and bottom 10 negative changes\n",
    "        top_positive_changes = [change for change in changes_list if change[1] > 0][:top_num]\n",
    "        bottom_negative_changes = [change for change in changes_list if change[1] < 0][-top_num:]\n",
    "        \n",
    "        # For each of the top 10 positive changes\n",
    "        for column_name, percent_change in top_positive_changes:\n",
    "            # Create a new row dictionary with positive fields\n",
    "            new_row = {\n",
    "                'category_column': row['CATEGORY_COLUMN'],\n",
    "                'category_value': row['CATEGORY_VALUE'],\n",
    "                'measure': measure,\n",
    "                'comparator': comparator,\n",
    "                'positive_negative': 'positive',\n",
    "                'column_name': column_name,\n",
    "                'value': percent_change\n",
    "            }\n",
    "            # Append to the list of new rows\n",
    "            new_rows.append(new_row)\n",
    "        \n",
    "        # For each of the bottom 10 negative changes\n",
    "        for column_name, percent_change in bottom_negative_changes:\n",
    "            # Create a new row dictionary with negative fields\n",
    "            new_row = {\n",
    "                'category_column': row['CATEGORY_COLUMN'],\n",
    "                'category_value': row['CATEGORY_VALUE'],\n",
    "                'measure': measure,\n",
    "                'comparator': comparator,\n",
    "                'positive_negative': 'negative',\n",
    "                'column_name': column_name,\n",
    "                'value': percent_change\n",
    "            }\n",
    "            # Append to the list of new rows\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "# Convert the list into a DataFrame\n",
    "categorical_summary_results_df = pd.DataFrame(new_rows)\n",
    "showall(categorical_summary_results_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78b67a0e-bc68-4a5d-aab9-67ea8d19b691",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#------------------------ Next Steps & To-Do-List -------------------------------------\n",
    "'''\n",
    "This script:\n",
    "- Add an 'ignore_columns' variable\n",
    "   -Either just one BIG list\n",
    "   -Or one big list AND lists for each function/scope that determines what it thinks 'should' be ignored (ie: IGNORE_LIST_GENERAL, IGNORE_LIST_classify_columns_for_corr_and_anova, IGNORE_LIST_corr_results, IGNORE_LIST_functional_categories, IGNORE_LIST_anova_results\n",
    "- ANOVA \n",
    "- Move 'add date sequence column' functionality to before initial data load\n",
    "   - Make it update the raw/upstream table in snowflake\n",
    "   - calculate # of occurances for each distinct date process value\n",
    "   - Sort distinct_values by # of occurances descending\n",
    "- Make parameters more flexible\n",
    "   -ID all 'user-specific' params\n",
    "   -Condense into fewer upstream params (like 'current_value' => main_metric_col_name)\n",
    "   -(Eventually convert UDF=>jupyter or parametrize input_table/view_name/output_table_name, for now just hard-code)\n",
    "   -test with our current view\n",
    "   -test with another view\n",
    "   -test with a table\n",
    "   -test with IRIS snowflake\n",
    "   -test with another test user on my pc\n",
    "   -test with another real user on their pc\n",
    "- Change printing to formatting 1 line (cleaner, see progress/completion more definitively)\n",
    "- Call 'add date normalization columns' UDF in snowflake from a placeholder cell/function before initial data load\n",
    "- Finish GPT error handler/planner\n",
    "- Turn this whole thing into a UI-Guided version... Super clean interface & GPT suggestions for direction/data inputs/etc.\n",
    "   -https://docs.google.com/document/d/13Bz-8-HgnPYTteV-o98E6dHKYlktx-wnxrjBxomSJy8/edit?usp=sharing\n",
    "   -https://medium.com/3blades-blog/jupyter-notebook-little-known-tricks-b0866a558017\n",
    "   -Add functionality to save variables to path/terminal variables. That way they are secure, and user doesn't have to manually input each time\n",
    "   -Brainstorm with Kris - General snowflake table architecture (how named, update_date/version data stored?, historical data stored?, etc)\n",
    "- Add GPT suggestions\n",
    "- Incorporate IA's existing analysis (likelihood of deceased, etc...)\n",
    "- \n",
    "\n",
    "\n",
    "\"Kris/Kari/Elaine Presentation\": https://docs.google.com/spreadsheets/d/1nXRHcNuCj2WpuzIPVz4YpE08k1MJw8ZWzImTDZOUHMU/edit#gid=0\n",
    "\n",
    "Planning tools\n",
    "- \"Big Project Execution Template - Work Project Chart\": https://aledade2-my.sharepoint.com/:x:/g/personal/miwilliams_aledade_com/EUj0DuaPLMxEm_N5CBZmc00BaevvffZ7lNxa9pJM3pKoUA?e=TAhF5h\n",
    "- \"Big Project Execution Template Personal Check-In Table\": https://aledade2-my.sharepoint.com/:x:/g/personal/miwilliams_aledade_com/EQLJgYx1ggNFlyRR4Q_zMH4B1fGSN87BqhS4AFqFLocxuA?e=7TqJft\n",
    "\n",
    "Snowflake:\n",
    "- Add $!!! (Easy & Impactful)\n",
    "\n",
    "Tableau:\n",
    "- Raw Analysis dump (most impacted fields for each step, compare with other categories)\n",
    "- Finish Pipeline View\n",
    "- Pipeline Step View\n",
    "- \n",
    "\n",
    "\n",
    "deets:\n",
    "Date Process Flows:\n",
    "    for every date field in my dataset I have two fields:\n",
    "    -the raw date field\n",
    "    -the 'normalized' date field that is compared to one of the more static/common/populated date fields (# of days before/after. negative if before, positive if after)\n",
    "    \n",
    "    example:\n",
    "    NON_APP_REFERRAL_DATE               [\"2023-08-03\",\"2023-10-24\",\"2023-03-30\",\"2023-...\n",
    "    NON_APP_REFERRAL_DATE_NORMALIZED    [-144,-304,-103,-203,-320,-84,-277,-187,-68,-3...\n",
    "    \n",
    "    I want to get a sense of the possible date \"process maps\"\n",
    "    \n",
    "    Here are my thoughts for goals/outcomes:\n",
    "    -identify ALL the possible general orders for dates (ie: [date1, date2, date3, date4], [date1, date3, date4], [date1, date4, date3] etc...)\n",
    "       -^Identify % for each as well (how frequently does each date order/flow happen?)\n",
    "       -this will give a sense of the big picture possible date/process flows\n",
    "    -Identify order likelihood for each date pair\n",
    "       -for the cartesian set all possible date column name pairs, in both orders (date1, date2), (date2, date1), (date1, date3)... compare both orders: % of  (date1, date2) compared to % of  (date2, date1) \n",
    "       -this will give a sense of the probability splits at each juncture\n",
    "    - Eventually, connect w/ enrolled, $, lost, potentially lost, etc...\n",
    "        - Calculate cost/value for each step\n",
    "        - Calculate all possible chains for best/worst case scenario (worst=>best: potentialy lost => enrolled), (best=>worst: almost enrolled => deceased)\n",
    "    - Create a tableau visual that shows the possible date processes\n",
    "    - Create a Tableau visual that shows our current distribution &:\n",
    "        - Expected # lost, maybe lost, enr rate, $...\n",
    "        - Trace\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5f84b29-d534-4b70-a454-8efd31d47a9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#------------------------ Documentation: Past Steps & Documents -------------------------------------\n",
    "#Cat Analysis Architecture\n",
    "\n",
    "# ------------------------Snowflake Pre-Work-------------------------------------\n",
    "'''\n",
    "1. Identified Craig's data sources\n",
    "2. Combined ALL I could find into one definition\n",
    "3. Turn View into clean/ordered Table, with Normalized dates, using JS UDF\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc997d2-26e2-49d4-a7fd-79198da6f2a4",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#------------------------ Jupyter Notebook Auto-Documentation (MUST BE AT BOTTOM!)-------------------------------------\n",
    "import os\n",
    "import nbformat\n",
    "import ast\n",
    "import inspect\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from IPython.core.getipython import get_ipython\n",
    "from datetime import datetime\n",
    "\n",
    "def jupyter_notebook_auto_documentation ():\n",
    "    # Step 1: Define the notebook path and get the base file name\n",
    "    notebook_path = os.path.join(os.getcwd(), 'Enroll Rate Data Analysis (Table Analysis).ipynb')\n",
    "    base_name = os.path.splitext(os.path.basename(notebook_path))[0]\n",
    "    \n",
    "    # Step 2: Load the notebook\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    # Step 3: Use IPython to get Python version\n",
    "    ipython = get_ipython()\n",
    "    python_version = ipython.run_cell('import platform\\nplatform.python_version()').result\n",
    "    \n",
    "    # Initialize containers for summary information\n",
    "    all_imports = set()\n",
    "    all_variables = defaultdict(set)\n",
    "    function_listing = defaultdict(list)\n",
    "    \n",
    "    # Get current date for file naming\n",
    "    update_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "    \n",
    "    # GlobalStructureMap file name\n",
    "    GlobalStructureMap_file_name = f\"{base_name}_GlobalStructureMap_{update_date}.txt\"\n",
    "    \n",
    "    # CellStructureMap file name\n",
    "    CellStructureMap_file_name = f\"{base_name}_CellStructureMap_{update_date}.txt\"\n",
    "    \n",
    "    # CellInteractionMap file name\n",
    "    CellInteractionMap_file_name = f\"{base_name}_CellInteractionMap_{update_date}.txt\"\n",
    "\n",
    "    \n",
    "    # Step 4: Parse notebook cells for overview information\n",
    "    for cell in nb['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            # Find imports using regex and add them to the set of all imports\n",
    "            imports = re.findall(r'^(?:import|from)\\s+(\\S+)', cell['source'], re.MULTILINE)\n",
    "            all_imports.update(imports)\n",
    "            \n",
    "            # AST analysis for functions and variables\n",
    "            tree = ast.parse(cell['source'])\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    function_listing[cell.get('execution_count', 'N/A')].append(node.name)\n",
    "                    # Check for local variables within functions\n",
    "                    for inner_node in ast.walk(node):\n",
    "                        if isinstance(inner_node, ast.Assign):\n",
    "                            for target in inner_node.targets:\n",
    "                                if isinstance(target, ast.Name):\n",
    "                                    all_variables[node.name].add(target.id)\n",
    "                elif isinstance(node, ast.Assign):\n",
    "                    # Consider top-level assignments as global variables\n",
    "                    for target in node.targets:\n",
    "                        if isinstance(target, ast.Name):\n",
    "                            all_variables['global'].add(target.id)\n",
    "    \n",
    "    # Save summary information to a text file\n",
    "    with open(GlobalStructureMap_file_name, 'w', encoding='utf-8') as summary_file:\n",
    "        # Overview section at the beginning\n",
    "        summary_file.write(\"Overview of GlobalStructureMap Document:\\n\")\n",
    "        summary_file.write(\"   - Structure: High-level overview including the environment setup.\\n\")\n",
    "        summary_file.write(\"   - Contents: Python version, imports, global variables, functions.\\n\")\n",
    "        summary_file.write(\"   - Use-Case: Quick understanding of dependencies and setup.\\n\\n\")\n",
    "        \n",
    "        # Table of Contents for quick navigation\n",
    "        summary_file.write(\"Table of Contents:\\n\")\n",
    "        summary_file.write(\"   1. Python Version\\n\")\n",
    "        summary_file.write(\"   2. Imports\\n\")\n",
    "        summary_file.write(\"   3. Variables\\n\")\n",
    "        summary_file.write(\"   4. Function Listings\\n\\n\")\n",
    "        \n",
    "        # How to interpret/use\n",
    "        summary_file.write(\"How to use/interpret this document:\\n\")\n",
    "        summary_file.write(\"   - Python Version: Ensures compatibility and environment setup.\\n\")\n",
    "        summary_file.write(\"   - Imports and Global Variables: Highlights external dependencies and key global data.\\n\")\n",
    "        summary_file.write(\"   - Functions List: A quick reference to functionalities defined within.\\n\\n\")\n",
    "        \n",
    "        # Variables\n",
    "        summary_file.write(f\"Python version: {python_version}\\n\")\n",
    "        summary_file.write(f\"All Imports: {sorted(all_imports)}\\n\")\n",
    "        summary_file.write(\"All Variables:\\n\")\n",
    "        for scope, vars in all_variables.items():\n",
    "            summary_file.write(f\"Scope: {scope}, Variables: {sorted(vars)}\\n\")\n",
    "        summary_file.write(\"Current values of important variables:\\n\")\n",
    "        summary_file.write(\"(Direct = used in current WIP, context = referenced...)\\n\")\n",
    "        \n",
    "        #current state of variables        \n",
    "        summary_file.write(f\"All Imports: {peek('1v')}\\n\")\n",
    "        summary_file.write(f\"All Imports: {peek('1p')}\\n\")\n",
    "        summary_file.write(f\"All Imports: {peek('2v')}\\n\")\n",
    "        summary_file.write(f\"All Imports: {peek('3p')}\\n\")\n",
    "        summary_file.write(f\"All Imports: {peek('3v')}\\n\")\n",
    "        summary_file.write(f\"All Imports: {peek('3p')}\\n\")\n",
    "        \n",
    "        summary_file.write(\"Cell Execution and Functions:\\n\")\n",
    "        # Modify the sorting line for function_listing.items() with a custom sort key\n",
    "        for exec_order, funcs in sorted(function_listing.items(), key=lambda x: (x[0] is None, x[0])):\n",
    "            summary_file.write(f\"Cell Execution Order: {exec_order}, Functions: {funcs}\\n\")\n",
    "\n",
    "    \n",
    "    # Detailed analysis part\n",
    "    with open(CellStructureMap_file_name, 'w', encoding='utf-8') as detailed_file:\n",
    "        # Overview section at the beginning\n",
    "        detailed_file.write(\"Overview of CellStructureMap Document:\\n\")\n",
    "        detailed_file.write(\"   - Structure: Detailed content mapping for each cell.\\n\")\n",
    "        detailed_file.write(\"   - Contents: Cell types, execution orders, imports, functions, variables, and integrations.\\n\")\n",
    "        detailed_file.write(\"   - Use-Case: Facilitates debugging and understanding cell roles and outputs.\\n\\n\")\n",
    "        # Table of Contents for quick navigation\n",
    "        detailed_file.write(\"Table of Contents:\\n\")\n",
    "        detailed_file.write(\"   - Cell Execution and Contents Overview\\n\\n\")\n",
    "        # How to interpret/use\n",
    "        detailed_file.write(\"How to use/interpret this document:\\n\")\n",
    "        detailed_file.write(\"   - Execution Order: Provides the sequence of notebook execution for logical flow.\\n\")\n",
    "        detailed_file.write(\"   - Detailed Cell Analysis: Offers an in-depth look at the code structure and data processing steps.\\n\\n\")\n",
    "    \n",
    "        # Content\n",
    "        for cell in nb['cells']:\n",
    "            detailed_file.write(f\"\\nCell type: {cell['cell_type']}, Execution order: {cell.get('execution_count', 'N/A')}\\n\")\n",
    "            \n",
    "            if cell['cell_type'] == 'code':\n",
    "                # Find imports using regex\n",
    "                imports = re.findall(r'^(?:import|from)\\s+(\\S+)', cell['source'], re.MULTILINE)\n",
    "                detailed_file.write(f\"Imports: {imports}\\n\")\n",
    "                \n",
    "                # AST analysis for functions and variables\n",
    "                tree = ast.parse(cell['source'])\n",
    "                for node in ast.walk(tree):\n",
    "                    if isinstance(node, ast.FunctionDef):\n",
    "                        detailed_file.write(f\"Function: {node.name}, Docstring: {ast.get_docstring(node)}\\n\")\n",
    "                        nested_funcs = [n.name for n in ast.walk(node) if isinstance(n, ast.FunctionDef) and n is not node]\n",
    "                        if nested_funcs:\n",
    "                            detailed_file.write(f\"Nested functions: {nested_funcs}\\n\")\n",
    "                    elif isinstance(node, ast.Assign):\n",
    "                        for target in node.targets:\n",
    "                            if isinstance(target, ast.Name):\n",
    "                                detailed_file.write(f\"Variable: {target.id}\\n\")\n",
    "                    elif isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):\n",
    "                        detailed_file.write(f\"Possible API call or Integration: {ast.dump(node)}\\n\")\n",
    "    \n",
    "    \n",
    "    # Containers for highly detailed information\n",
    "    classes = defaultdict(list)\n",
    "    async_functions = defaultdict(list)\n",
    "    decorators = defaultdict(list)\n",
    "    comprehensions = defaultdict(list)\n",
    "    function_calls_details = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Parsing for highly detailed information\n",
    "    for cell_index, cell in enumerate(nb['cells']):\n",
    "        if cell['cell_type'] == 'code':\n",
    "            tree = ast.parse(cell['source'])\n",
    "            for node in ast.walk(tree):\n",
    "                # Classes\n",
    "                if isinstance(node, ast.ClassDef):\n",
    "                    classes[cell_index].append(node.name)\n",
    "                # Async Functions\n",
    "                elif isinstance(node, ast.AsyncFunctionDef):\n",
    "                    async_functions[cell_index].append(node.name)\n",
    "                # Decorators\n",
    "                elif isinstance(node, ast.FunctionDef) or isinstance(node, ast.AsyncFunctionDef):\n",
    "                    for decorator in node.decorator_list:\n",
    "                        decorator_name = getattr(decorator, 'id', str(decorator))\n",
    "                        decorators[cell_index].append(decorator_name)\n",
    "                # Comprehensions\n",
    "                elif isinstance(node, (ast.ListComp, ast.DictComp, ast.SetComp, ast.GeneratorExp)):\n",
    "                    comprehensions[cell_index].append(ast.dump(node))\n",
    "                # Function Calls\n",
    "                elif isinstance(node, ast.Call):\n",
    "                    callee = getattr(node.func, 'id', getattr(node.func, 'attr', str(node.func)))\n",
    "                    function_calls_details[cell_index]['calls'].append(callee)\n",
    "    \n",
    "    # Writing highly detailed analysis to file\n",
    "    with open(CellInteractionMap_file_name, 'w', encoding='utf-8') as f:\n",
    "        # Overview section at the beginning\n",
    "        f.write(\"Overview of CellInteractionMap Document:\\n\")\n",
    "        f.write(\"   - Structure: Examination of cell interconnections and relationships.\\n\")\n",
    "        f.write(\"   - Contents: Classes, async functions, decorators, comprehensions, function calls.\\n\")\n",
    "        f.write(\"   - Use-Case: Essential for data flow, dependencies, and interaction optimization.\\n\\n\")\n",
    "        # Table of Contents for quick navigation\n",
    "        f.write(\"Table of Contents:\\n\")\n",
    "        f.write(\"   1. Classes\\n\")\n",
    "        f.write(\"   2. Async Functions\\n\")\n",
    "        f.write(\"   3. Decorators\\n\")\n",
    "        f.write(\"   4. Comprehensions\\n\")\n",
    "        f.write(\"   5. Function Calls\\n\\n\")\n",
    "        # How to interpret/use\n",
    "        f.write(\"How to use/interpret this document:\\n\")\n",
    "        f.write(\"   - Inter-cell Relationships: Unveils how different components work together, enhancing comprehension of complex notebooks.\\n\")\n",
    "        f.write(\"   - Insights for Optimization: Identifies potential refactoring opportunities for efficiency or readability improvements.\\n\\n\")\n",
    "    \n",
    "        \n",
    "        #Content\n",
    "        \n",
    "        # Classes\n",
    "        if classes:\n",
    "            f.write(\"Classes:\\n\")\n",
    "            for cell, class_names in classes.items():\n",
    "                f.write(f\"Cell {cell}: {', '.join(class_names)}\\n\")\n",
    "        # Async Functions\n",
    "        if async_functions:\n",
    "            f.write(\"\\nAsync Functions:\\n\")\n",
    "            for cell, functions in async_functions.items():\n",
    "                f.write(f\"Cell {cell}: {', '.join(functions)}\\n\")\n",
    "        # Decorators\n",
    "        if decorators:\n",
    "            f.write(\"\\nDecorators:\\n\")\n",
    "            for cell, decorator_names in decorators.items():\n",
    "                f.write(f\"Cell {cell}: {', '.join(decorator_names)}\\n\")\n",
    "        # Comprehensions\n",
    "        if comprehensions:\n",
    "            f.write(\"\\nComprehensions:\\n\")\n",
    "            for cell, comps in comprehensions.items():\n",
    "                f.write(f\"Cell {cell}: {', '.join(comps)}\\n\")\n",
    "        # Function Calls\n",
    "        if function_calls_details:\n",
    "            f.write(\"\\nFunction Calls:\\n\")\n",
    "            for cell, details in function_calls_details.items():\n",
    "                f.write(f\"Cell {cell}: {', '.join(details['calls'])}\\n\")\n",
    "\n",
    "jupyter_notebook_auto_documentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb297ff-97d9-4ccb-97e2-e106909b1538",
   "metadata": {},
   "source": [
    "## Categorical Analysis (Need to convert to functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f0b22-c02b-4ded-9c96-e0df09ec7c74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#------------------------ Categorical Analysis - Data Generation -------------------------------------\n",
    "#to add later:\n",
    "#   -distribution (to column & compare here for both continuous & categorical)\n",
    "#   -variability changes/analysis\n",
    "\n",
    "experiment_category_columns = None\n",
    "experiment_category_list = None\n",
    "cat_test_results_dict = None\n",
    "def categorical_analysis ():\n",
    "    print(\"categorical_analysis starting...\")\n",
    "    #__Needed from previous functions__\n",
    "    #--get_or_create_session--\n",
    "    global session\n",
    "    #--load_data--\n",
    "    global enroll_stable_copy\n",
    "    #--upload_column_tests_to_snowflake--\n",
    "    global snowflake_updated_column_analysis\n",
    "    \n",
    "    #__Set for the first time in this function__\n",
    "    global experiment_category_columns\n",
    "    global experiment_category_list\n",
    "    global cat_test_results_dict\n",
    "    \n",
    "    #___function logic starts here___\n",
    "    if snowflake_updated_column_analysis is not None:\n",
    "        print(\"Column Analysis Results in Local Storage\")\n",
    "    else:\n",
    "        print(\"Checking for Analysis Results Table in Snowflake\")\n",
    "\n",
    "        print(\"If not in snowflake...\")\n",
    "        print(\"Calling Column Analysis Function...\")\n",
    "\n",
    "    columns_to_pull = [\"COLUMN_NAME\", \"TEST_DATA_TYPE\", \"DISTINCT_COUNT\", \"DISTINCT_VALUES\"]\n",
    "    new_column_names = {\n",
    "        \"COLUMN_NAME\": \"CATEGORICAL_FIELD\",\n",
    "        \"TEST_DATA_TYPE\": \"CATEGORICAL_FIELD_TYPE\",\n",
    "        \"DISTINCT_COUNT\": \"NUM_CATEGORIES\",\n",
    "        \"DISTINCT_VALUES\": \"CATEGORIES_LIST\"\n",
    "    }\n",
    "    \n",
    "    # Convert 'DISTINCT_COUNT' column to numeric, errors='coerce' will convert non-convertible values to NaN\n",
    "    snowflake_updated_column_analysis['DISTINCT_COUNT'] = pd.to_numeric(snowflake_updated_column_analysis['DISTINCT_COUNT'], errors='coerce')\n",
    "    \n",
    "    # Now you can filter the DataFrame\n",
    "    potentially_categorical_df = snowflake_updated_column_analysis[snowflake_updated_column_analysis['DISTINCT_COUNT'] <= 10][columns_to_pull]\n",
    "    \n",
    "    # Rename the columns to the new names\n",
    "    potentially_categorical_df = potentially_categorical_df.rename(columns=new_column_names)\n",
    "\n",
    "    print(\"len is:\",potentially_categorical_df.shape[0])\n",
    "    #pd.options.display.max_columns = 60\n",
    "    #pd.options.display.max_rows = 180\n",
    "    #display(potentially_categorical_df)\n",
    "    #pd.options.display.max_columns = 60\n",
    "    #pd.options.display.max_rows = 20\n",
    "\n",
    "    experiment_category_columns = ['CURRENT_PIPELINE_STEP']\n",
    "\n",
    "    cat_test_results_dict = {}\n",
    "    for experiment_category_column in experiment_category_columns:\n",
    "        \n",
    "        print(\"experiment_category_column is:\")\n",
    "        print(experiment_category_column)\n",
    "        experiment_category_list = potentially_categorical_df[potentially_categorical_df['CATEGORICAL_FIELD'] == experiment_category_column]['CATEGORIES_LIST']\n",
    "        experiment_category_list = ast.literal_eval(experiment_category_list.iloc[0])\n",
    "        print(experiment_category_list)\n",
    "        for experiment_category_value in experiment_category_list:\n",
    "            print(\"experiment_category_value is:\")\n",
    "            print(experiment_category_value)\n",
    "\n",
    "            print(\"running column analysis on cat...\")\n",
    "            category_subset_results_raw = enroll_stable_copy[enroll_stable_copy[experiment_category_column] == experiment_category_value]\n",
    "            category_subset_results = run_column_analysis(category_subset_results_raw)\n",
    "            cat_test_results_dict[experiment_category_value] = category_subset_results\n",
    "            #display(category_subset_results)\n",
    "            print(\"cat column analysis complete\")\n",
    "\n",
    "            print(\"running column analysis on inverse...\")\n",
    "            category_subset_results_inverse_raw = enroll_stable_copy[enroll_stable_copy[experiment_category_column] != experiment_category_value]\n",
    "            category_subset_results_inverse = run_column_analysis(category_subset_results_inverse_raw)\n",
    "            cat_test_results_dict[experiment_category_value + \"inverse\"] = category_subset_results_inverse\n",
    "            #display(category_subset_results_inverse)\n",
    "            print(\"inverse column analysis complete\")\n",
    "            \n",
    "        print(\"done\")\n",
    "            \n",
    "categorical_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee046e-3d9b-4b8e-971d-e5e71a2b9576",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------ Categorical Analysis - Comparison -------------------------------------\n",
    "#experiment_category_columns = None\n",
    "#experiment_category_list = None\n",
    "#cat_test_results_dict = None\n",
    "\n",
    "#pd.options.display.max_columns = 60\n",
    "#pd.options.display.max_rows = 180\n",
    "#display(potentially_categorical_df)\n",
    "#pd.options.display.max_columns = 60\n",
    "#pd.options.display.max_rows = 20\n",
    "\n",
    "#print(experiment_category_columns)\n",
    "#print(experiment_category_list)\n",
    "#print(cat_test_results_dict)\n",
    "\n",
    "snowflake_updated_column_analysis_copy = snowflake_updated_column_analysis.set_index(\"COLUMN_NAME\")\n",
    "\n",
    "# Initialize an empty list to store row dictionaries\n",
    "rows_list = []\n",
    "\n",
    "for experiment_category_column in experiment_category_columns:\n",
    "    print(\"CATEGORY_COLUMN:\",experiment_category_column)\n",
    "    for experiment_category_value in experiment_category_list:\n",
    "        print(\"CATEGORY_VALUE:\",experiment_category_value)\n",
    "        \n",
    "        cat_results = cat_test_results_dict[experiment_category_value]\n",
    "        #cat_results = cat_results.set_index(\"COLUMN_NAME\")\n",
    "        cat_results = cat_results.set_index(\"COLUMN_NAME\")\n",
    "        #showall(cat_results)\n",
    "        #display(cat_results)\n",
    "        \n",
    "        cat_inverse_results = cat_test_results_dict[experiment_category_value + \"inverse\"]\n",
    "        cat_inverse_results = cat_inverse_results.set_index(\"COLUMN_NAME\")\n",
    "        \n",
    "        #cat_inverse_results = cat_inverse_results.set_index(\"COLUMN_NAME\")\n",
    "        #display(cat_inverse_results)\n",
    "        \n",
    "        diff = cat_results.compare(cat_inverse_results)\n",
    "\n",
    "        \n",
    "        cat_columns = set(cat_results.columns)\n",
    "        cat_rows = set(cat_results.index)#set(cat_results.loc[list(cat_results.index), \"COLUMN_NAME\"])\n",
    "\n",
    "        cat_inv_columns = set(cat_inverse_results.columns)\n",
    "        cat_inv_rows = set(cat_inverse_results.index)#set(cat_inverse_results.loc[list(cat_inverse_results.index), \"COLUMN_NAME\"])\n",
    "\n",
    "        diff_columns = set([col[0] for col in diff.columns])\n",
    "        diff_rows = set(diff.index)\n",
    "        #cat_rows = set(diff.loc[list(diff.index), \"COLUMN_NAME\"])\n",
    "        #cat_rows = set(diff.loc[diff.index, \"COLUMN_NAME\"])\n",
    "\n",
    "        #CAT_COLUMNS_NOT_IN_DIFF     = cat_results_columns - diff_columns\n",
    "        #CAT_INV_COLUMNS_NOT_IN_DIFF = cat_inv_columns - diff_columns\n",
    "        #CAT_ROWS_NOT_IN_DIFF        = cat_rows - diff_rows\n",
    "        #CAT_INV_ROWS_NOT_IN_DIFF    = cat_inv_rows - diff_rows\n",
    "        \n",
    "        _0_percent_null_fields = cat_results[cat_results['NULL_PERCENTAGE'] == 0].index.tolist()        \n",
    "        _0_percent_null_fields_count = len(_0_percent_null_fields)\n",
    "\n",
    "        _100_percent_null_fields = cat_results[cat_results['NULL_PERCENTAGE'] == 1].index.tolist()\n",
    "        _100_percent_null_fields_count = len(_100_percent_null_fields)\n",
    "    \n",
    "        columns_to_analyze = ['NULL_PERCENTAGE', 'NUMBER_AVG','BINARY_PERCENT_TRUE_OR_1','BINARY_PERCENT_FALSE_OR_0','STRING_AVG_LENGTH']\n",
    "        #DISTINCT_VALUES\n",
    "        #DATE_AVG \n",
    "        \n",
    "        # Utility function for calculating percent change\n",
    "        def calculate_percent_change(before, after):\n",
    "            try:\n",
    "                before_numeric = float(before) if before is not None else None\n",
    "                after_numeric = float(after) if after is not None else None\n",
    "\n",
    "                # Now check for np.isnan, since we have ensured the values are either None or float\n",
    "                before_numeric = None if before_numeric is not None and np.isnan(before_numeric) else before_numeric\n",
    "                after_numeric = None if after_numeric is not None and np.isnan(after_numeric) else after_numeric\n",
    "                \n",
    "                if before_numeric is not None and after_numeric is not None and before_numeric != 0:\n",
    "                    return (after_numeric - before_numeric) / before_numeric\n",
    "            except ValueError:\n",
    "                pass\n",
    "            return None\n",
    "        \n",
    "        # Initialize a dictionary to hold all results\n",
    "        analysis_results = {}\n",
    "        \n",
    "        for column in columns_to_analyze:\n",
    "            # Extract column-specific dictionaries from DataFrames\n",
    "            column_original = snowflake_updated_column_analysis_copy[column].to_dict()\n",
    "            column_cat = cat_results[column].to_dict()\n",
    "            column_cat_inverse = cat_inverse_results[column].to_dict()\n",
    "        \n",
    "            # Prepare storage within analysis_results\n",
    "            analysis_results[f'percent_change_in_{column}_compared_to_inverse'] = {}\n",
    "            analysis_results[f'percent_change_in_{column}_compared_to_original'] = {}\n",
    "            analysis_results[f'columns_where_{column}_did_change_compared_to_inverse'] = []\n",
    "            analysis_results[f'columns_where_{column}_didnt_change_compared_to_inverse'] = []\n",
    "            analysis_results[f'columns_where_{column}_did_change_compared_to_original'] = []\n",
    "            analysis_results[f'columns_where_{column}_didnt_change_compared_to_original'] = []\n",
    "        \n",
    "            # Combined loop for both comparisons\n",
    "            for column_name in column_cat.keys():\n",
    "                cat_value = column_cat[column_name]\n",
    "                inv_value = column_cat_inverse.get(column_name)\n",
    "                orig_value = column_original.get(column_name)\n",
    "        \n",
    "                # Calculate percent changes with numeric conversion\n",
    "                percent_change_to_inv = calculate_percent_change(cat_value, inv_value)\n",
    "                percent_change_to_orig = calculate_percent_change(cat_value, orig_value)\n",
    "        \n",
    "                # Store results if percent change is not None and not 0\n",
    "                if percent_change_to_inv not in (None, 0):\n",
    "                    analysis_results[f'percent_change_in_{column}_compared_to_inverse'][column_name] = {'cat_value': cat_value, 'inv_value': inv_value, 'percent_change': percent_change_to_inv}\n",
    "                    analysis_results[f'columns_where_{column}_did_change_compared_to_inverse'].append(column_name)\n",
    "                else:\n",
    "                    analysis_results[f'columns_where_{column}_didnt_change_compared_to_inverse'].append(column_name)\n",
    "        \n",
    "                if percent_change_to_orig not in (None, 0):\n",
    "                    analysis_results[f'percent_change_in_{column}_compared_to_original'][column_name] = {'cat_value': cat_value, 'original_value': orig_value, 'percent_change': percent_change_to_orig}\n",
    "                    analysis_results[f'columns_where_{column}_did_change_compared_to_original'].append(column_name)\n",
    "                else:\n",
    "                    analysis_results[f'columns_where_{column}_didnt_change_compared_to_original'].append(column_name)\n",
    "            analysis_results[f'percent_change_in_{column}_compared_to_inverse'] = dict(sorted(analysis_results[f'percent_change_in_{column}_compared_to_inverse'].items(), key=lambda item: item[1]['percent_change'], reverse=True))\n",
    "            analysis_results[f'percent_change_in_{column}_compared_to_original'] = dict(sorted(analysis_results[f'percent_change_in_{column}_compared_to_original'].items(), key=lambda item: item[1]['percent_change'], reverse=True))\n",
    "\n",
    "\n",
    "        '''\n",
    "        columns_to_verify = ['NULL_PERCENTAGE', 'NUMBER_AVG']\n",
    "        \n",
    "        # Printing the analysis results for percent changes\n",
    "        for column in columns_to_verify:\n",
    "            print(f\"Analysis results for {column}:\")\n",
    "        \n",
    "            # Define keys for percent changes\n",
    "            percent_change_keys = [\n",
    "                f'percent_change_in_{column}_compared_to_inverse',\n",
    "                f'percent_change_in_{column}_compared_to_original'\n",
    "            ]\n",
    "        \n",
    "            # Iterate through the specific keys for percent changes\n",
    "            for key in percent_change_keys:\n",
    "                print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
    "        \n",
    "                # Check if the key exists to avoid KeyError\n",
    "                if key in analysis_results:\n",
    "                    for col_name, change_details in analysis_results[key].items():\n",
    "                        print(f\"  {col_name}: {change_details}\")\n",
    "                else:\n",
    "                    print(\"  No data available.\")\n",
    "        \n",
    "            print(\"\\n\")  # Add an extra newline for better readability between column results\n",
    "        '''\n",
    "\n",
    "        # Create a dictionary for the row\n",
    "        row_dict = {\n",
    "            \"CATEGORY_COLUMN\" : experiment_category_column,\n",
    "            \"CATEGORY_VALUE\": experiment_category_value,\n",
    "            #\"CAT_COLUMNS_NOT_IN_DIFF\": CAT_COLUMNS_NOT_IN_DIFF,\n",
    "            #\"CAT_INV_COLUMNS_NOT_IN_DIFF\": CAT_INV_COLUMNS_NOT_IN_DIFF,\n",
    "            #\"CAT_ROWS_NOT_IN_DIFF\": CAT_ROWS_NOT_IN_DIFF,\n",
    "            #\"CAT_INV_ROWS_NOT_IN_DIFF\": CAT_INV_ROWS_NOT_IN_DIFF,\n",
    "            \"_0_percent_null_fields\": _0_percent_null_fields,\n",
    "            \"_0_percent_null_fields_count\": _0_percent_null_fields_count,\n",
    "            \"_100_percent_null_fields\": _100_percent_null_fields,\n",
    "            \"_100_percent_null_fields_count\": _100_percent_null_fields_count\n",
    "        }\n",
    "        # Update row_dict by merging analysis_results into it\n",
    "        row_dict.update(analysis_results)\n",
    "        \n",
    "        # Append the row dictionary to the list\n",
    "        rows_list.append(row_dict)\n",
    "\n",
    "\n",
    "        #print(cat_results_columns)\n",
    "        #print(cat_results_rows)\n",
    "        #print(cat_inverse_results_columns)\n",
    "        #print(cat_inverse_results_rows)  \n",
    "\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "stats_df = pd.DataFrame(rows_list)\n",
    "\n",
    "# Display the new DataFrame\n",
    "display(stats_df.iloc[:, :12]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
